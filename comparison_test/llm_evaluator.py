"""
LLM-Based Schema Evaluator
===========================
Basit ve anlaÅŸÄ±lÄ±r API kullanÄ±mÄ± ile medikal ÅŸema deÄŸerlendirme

Desteklenen modeller:
- Gemini (Google AI Studio)
- Gemma (Google AI Studio) 
- GLM (ZhipuAI)
- DeepSeek
- ve daha fazlasÄ±...

KULLANIM:
    # API keylerini tanÄ±mla
    API_KEYS = {
        "gemini": "AIzaSy...",
        "gemma": "KGAT_...",
        "glm": "sk-...",
        "deepseek": "sk-..."
    }
    
    # Evaluator oluÅŸtur
    evaluator = LLMEvaluator(
        model_type="gemini",
        model_name="gemini-1.5-flash",
        api_key=API_KEYS["gemini"]
    )
    
    # ÅemalarÄ± deÄŸerlendir
    result = evaluator.evaluate_schema_pair(ground_truth, prediction, input_text)
"""

import json
import os
import time
from typing import Dict, List, Any, Optional
import logging

# Google AI iÃ§in
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš  google-generativeai kurulu deÄŸil. Gemini/Gemma kullanmak iÃ§in:")
    print("  pip install google-generativeai")

# OpenAI-compatible API'ler iÃ§in
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš  openai kurulu deÄŸil. GLM/DeepSeek kullanmak iÃ§in:")
    print("  pip install openai")

logger = logging.getLogger(__name__)


class LLMEvaluator:
    """
    Basit ve anlaÅŸÄ±lÄ±r LLM-based ÅŸema deÄŸerlendirici
    
    Bu class direkt olarak API'leri kullanÄ±r, karmaÅŸÄ±k config yok!
    """
    
    # Her model iÃ§in rate limit ayarlarÄ± (saniye cinsinden bekleme sÃ¼resi)
    RATE_LIMITS = {
        "gemini-2.5-pro": 60 / 1,          # 1 req/min
        "gemini-2.5-flash": 60 / 5,      # 5 req/min = 12 sn bekle
        "gemini-2.5-flash-lite": 60 / 10, # 10 req/min = 6 sn bekle
        "gemini-1.5-flash": 60 / 15,      # 15 req/min = 4 sn bekle
        "gemini-1.5-pro": 60 / 2,         # 2 req/min = 30 sn bekle
        "gemma-3-27b-it": 60 / 10,        # 10 req/min = 6 sn bekle
        "glm-4-flash": 1.0,               # GÃ¼venli aralÄ±k
        "deepseek-chat": 1.0,             # GÃ¼venli aralÄ±k
    }
    
    def __init__(self, 
                 model_type: str,
                 model_name: str,
                 api_key: str):
        """
        LLM Evaluator'Ä± baÅŸlat
        
        Args:
            model_type: "gemini", "gemma", "glm", "deepseek", vb.
            model_name: "gemini-1.5-flash", "gemma-3-27b-it", vb.
            api_key: API anahtarÄ±nÄ±z
            
        Ã–rnek:
            evaluator = LLMEvaluator("gemini", "gemini-1.5-flash", "AIzaSy...")
        """
        self.model_type = model_type.lower()
        self.model_name = model_name
        self.api_key = api_key
        
        # Rate limiting iÃ§in bekleme sÃ¼resi
        self.sleep_time = self.RATE_LIMITS.get(model_name, 1.0)
        
        # Model'i baÅŸlat
        self._initialize_model()
        
        print(f"âœ“ {model_type} modeli baÅŸlatÄ±ldÄ±: {model_name}")
        print(f"  Rate limit: Her istek arasÄ± {self.sleep_time:.1f} saniye bekleme")
    
    def _initialize_model(self):
        """SeÃ§ilen modeli baÅŸlat"""
        
        if self.model_type in ["gemini", "gemma"]:
            if not GEMINI_AVAILABLE:
                raise ImportError("google-generativeai kurulu deÄŸil!")
            
            genai.configure(api_key=self.api_key)
            
            # Model adÄ±nÄ± dÃ¼zelt (gemma iÃ§in models/ prefix gerekli)
            if self.model_type == "gemma":
                model_path = f"models/{self.model_name}"
            else:
                model_path = self.model_name
            
            self.model = genai.GenerativeModel(model_path)
            
        elif self.model_type in ["glm", "deepseek"]:
            if not OPENAI_AVAILABLE:
                raise ImportError("openai kurulu deÄŸil!")
            
            # API base URL'leri
            base_urls = {
                "glm": "https://open.bigmodel.cn/api/paas/v4/",
                "deepseek": "https://api.deepseek.com"
            }
            
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=base_urls.get(self.model_type)
            )
        
        else:
            raise ValueError(f"Desteklenmeyen model tipi: {self.model_type}")
            print("Desteklenen modeller: gemini, gemma, glm, deepseek")
    
    def evaluate_schema_pair(self, 
                           ground_truth: Dict,
                           prediction: Dict,
                           input_text: str) -> Dict[str, Any]:
        """
        Ä°ki ÅŸemayÄ± karÅŸÄ±laÅŸtÄ±r ve deÄŸerlendir
        
        Args:
            ground_truth: Referans ÅŸema (schema_train.json'dan)
            prediction: Modelinizin Ã¼rettiÄŸi ÅŸema
            input_text: Orijinal radyoloji metni
            
        Returns:
            DeÄŸerlendirme sonuÃ§larÄ±:
            {
                "similarity_score": 0.0-1.0,
                "clinical_equivalence": "exact|high|partial|low",
                "are_same_meaning": True/False,
                "entity_level_analysis": [...],
                "critical_errors": [...],
                "minor_differences": [...],
                "overall_assessment": "..."
            }
        """
        # DeÄŸerlendirme promptunu hazÄ±rla
        prompt = self._build_evaluation_prompt(ground_truth, prediction, input_text)
        
        # LLM'e sor
        response = self._generate(prompt)
        
        # CevabÄ± parse et
        result = self._parse_json_response(response)
        
        # Rate limit iÃ§in bekle
        time.sleep(self.sleep_time)
        
        return result
    
    def batch_evaluate(self, 
                      comparisons: List[Dict],
                      save_every: int = 10) -> List[Dict]:
        """
        Birden fazla ÅŸema Ã§iftini deÄŸerlendir
        
        Args:
            comparisons: [{"ground_truth": {...}, "prediction": {...}, "input": "..."}, ...]
            save_every: Her N deÄŸerlendirmede ara sonucu kaydet
            
        Returns:
            TÃ¼m deÄŸerlendirme sonuÃ§larÄ± listesi
        """
        results = []
        
        print(f"\nğŸ”„ {len(comparisons)} ÅŸema Ã§ifti deÄŸerlendiriliyor...")
        print(f"â± Tahmini sÃ¼re: {len(comparisons) * self.sleep_time / 60:.1f} dakika")
        
        for idx, comp in enumerate(comparisons):
            try:
                result = self.evaluate_schema_pair(
                    comp['ground_truth'],
                    comp['prediction'],
                    comp['input']
                )
                results.append(result)
                
                # Ä°lerleme gÃ¶ster
                if (idx + 1) % 5 == 0:
                    print(f"  âœ“ {idx + 1}/{len(comparisons)} tamamlandÄ±")
                
                # Ara sonuÃ§larÄ± kaydet
                if (idx + 1) % save_every == 0:
                    self._save_intermediate(results, idx + 1)
                
            except Exception as e:
                print(f"  âš  Hata (sample {idx}): {e}")
                results.append({
                    'error': str(e),
                    'sample_idx': idx
                })
        
        print(f"âœ“ DeÄŸerlendirme tamamlandÄ±!")
        return results
    def _safe_extract_gemini_text(self, response) -> Optional[str]:
        """
        Gemini response'tan gÃ¼venli ÅŸekilde text Ã§Ä±kar
        (finish_reason=2 gibi durumlarÄ± tolere eder)
        """
        try:
            if not hasattr(response, "candidates") or not response.candidates:
                return None

            candidate = response.candidates[0]

            if not hasattr(candidate, "content") or not candidate.content:
                return None

            parts = candidate.content.parts
            if not parts:
                return None

            texts = []
            for part in parts:
                if hasattr(part, "text") and part.text:
                    texts.append(part.text)

            return "\n".join(texts) if texts else None

        except Exception:
            return None


    def _generate(self, prompt: str) -> str:
        """
        SeÃ§ilen model ile text Ã¼ret
        
        Her model tipi iÃ§in farklÄ± API Ã§aÄŸrÄ±sÄ±
        """
        try:
            if self.model_type in ["gemini", "gemma"]:
                return self._generate_gemini(prompt)
            
            elif self.model_type in ["glm", "deepseek"]:
                return self._generate_openai_compatible(prompt)
            
            else:
                raise ValueError(f"Model tipi desteklenmiyor: {self.model_type}")
        
        except Exception as e:
            logger.error(f"LLM generation hatasÄ±: {e}")
            raise
    
    def _generate_gemini(self, prompt: str) -> str:
        """Gemini/Gemma iÃ§in Ã¼retim (SAFE)"""

        generation_config = {
            "temperature": 0.1,
            "top_p": 0.85,
            "max_output_tokens": 2048
        }

        response = self.model.generate_content(
            prompt,
            generation_config=generation_config
        )

        text = self._safe_extract_gemini_text(response)

        if text is None:
            finish_reason = (
                response.candidates[0].finish_reason
                if hasattr(response, "candidates") and response.candidates
                else "unknown"
            )
            raise RuntimeError(
                f"Empty / blocked Gemini response (finish_reason={finish_reason})"
            )

        return text.strip()

    
    def _generate_openai_compatible(self, prompt: str) -> str:
        """GLM, DeepSeek gibi OpenAI-compatible API'ler iÃ§in Ã¼retim"""
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=2048
        )
        
        return response.choices[0].message.content.strip()
    
    def _build_evaluation_prompt(self, 
                                 gt: Dict, 
                                 pred: Dict, 
                                 input_text: str) -> str:
        """
        DeÄŸerlendirme promptunu oluÅŸtur
        
        LLM'e hangi kriterlere gÃ¶re deÄŸerlendireceÄŸini sÃ¶ylÃ¼yoruz
        """
        
        # Ground truth ve prediction'Ä± JSON string'e Ã§evir
        gt_output = json.dumps(gt.get('output', []), indent=2)
        pred_output = json.dumps(pred.get('output', []), indent=2)
        
        prompt = f"""You are an expert medical information extraction evaluator. Compare two medical schema extractions and determine if they are clinically equivalent.IMPORTANT:
- This is a text consistency evaluation task.
- Do NOT provide medical advice or diagnosis.
- Do NOT suggest treatment.


ğŸ“‹ INPUT RADIOLOGY TEXT:
{input_text}

ğŸ“Š GROUND TRUTH SCHEMA (Reference):
{gt_output}

ğŸ¤– PREDICTED SCHEMA (System Output):
{pred_output}

ğŸ“ EVALUATION CRITERIA:

1. **Clinical Equivalence**
   - Do both schemas convey the same clinical information?
   - Consider: synonyms (e.g., "enlarged" = "splenomegaly")
   - Consider: paraphrasing (e.g., "no fracture" = "fracture absent")

2. **Entity Completeness**
   - Are all entities from ground truth captured?
   - Missing entities = critical error
   - Extra valid entities = acceptable

3. **Field Accuracy** (for each field):
   - abnormality: exact match best, semantic match acceptable
   - presence: CRITICAL - must match exactly (present/absent/uncertain)
   - location: consider anatomical synonyms
   - degree: consider equivalent descriptions
   - measurement: exact numbers important

4. **Clinical Impact**
   - Which differences are clinically significant?
   - Which are just formatting/wording differences?

ğŸ¯ RESPOND IN THIS EXACT JSON FORMAT (no additional text):
{{
  "similarity_score": <float 0.0 to 1.0>,
  "clinical_equivalence": "<exact|high|partial|low>",
  "are_same_meaning": <true or false>,
  "entity_level_analysis": [
    {{
      "gt_entity_idx": <int>,
      "pred_entity_idx": <int or null>,
      "field_scores": {{
        "abnormality": <float 0.0-1.0>,
        "presence": <float 0.0-1.0>,
        "location": <float 0.0-1.0>,
        "degree": <float 0.0-1.0>,
        "measurement": <float 0.0-1.0>
      }},
      "notes": "<brief explanation>"
    }}
  ],
  "critical_errors": [<list of critical mistakes>],
  "minor_differences": [<list of acceptable variations>],
  "overall_assessment": "<brief summary in 1-2 sentences>"
}}

IMPORTANT RULES:
- Be strict on presence/absence (critical for diagnosis)
- Be lenient on synonyms and paraphrasing
- Focus on clinical impact, not textual similarity
- Return ONLY valid JSON, no markdown, no extra text
"""
        
        return prompt
    
    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """
        LLM cevabÄ±nÄ± parse et
        
        BazÄ± LLM'ler markdown ile JSON dÃ¶ndÃ¼rÃ¼r (```json ... ```),
        bunlarÄ± temizleyip parse ediyoruz
        """
        # Temizle
        response = response.strip()
        
        # Markdown code block varsa kaldÄ±r
        if response.startswith('```json'):
            response = response.replace('```json', '', 1)
        if response.startswith('```'):
            response = response.replace('```', '', 1)
        if response.endswith('```'):
            response = response.rsplit('```', 1)[0]
        
        response = response.strip()
        
        # JSON parse et
        try:
            parsed = json.loads(response)
            return parsed
        
        except json.JSONDecodeError as e:
            print(f"âš  JSON parse hatasÄ±: {e}")
            print(f"Raw response (ilk 500 char): {response[:500]}")
            
            # Hata durumunda minimal bir yapÄ± dÃ¶ndÃ¼r
            return {
                'similarity_score': 0.0,
                'clinical_equivalence': 'unknown',
                'are_same_meaning': False,
                'entity_level_analysis': [],
                'critical_errors': ['JSON parse failed'],
                'minor_differences': [],
                'overall_assessment': 'Evaluation failed due to parse error',
                'raw_response': response
            }
    
    def _save_intermediate(self, results: List[Dict], count: int):
        """Ara sonuÃ§larÄ± kaydet (hata durumunda kaybetmemek iÃ§in)"""
        filename = f'llm_eval_intermediate_{count}.json'
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ’¾ Ara sonuÃ§ kaydedildi: {filename}")


# ============================================================================
# Semantic Similarity (Embedding-based)
# ============================================================================

class EmbeddingBasedEvaluator:
    """
    Sentence embeddings kullanarak semantik benzerlik hesapla
    
    Bu method API Ã§aÄŸrÄ±sÄ± gerektirmez, tamamen local Ã§alÄ±ÅŸÄ±r!
    """
    
    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):
        """
        Args:
            model_name: HuggingFace model adÄ±
                       - 'sentence-transformers/all-MiniLM-L6-v2' (genel, hÄ±zlÄ±)
                       - 'dmis-lab/biobert-base-cased-v1.2' (medikal, daha iyi)
        """
        try:
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(model_name)
            print(f"âœ“ Embedding modeli yÃ¼klendi: {model_name}")
        except ImportError:
            print("âš  sentence-transformers kurulu deÄŸil!")
            print("  pip install sentence-transformers")
            self.model = None
    
    def compute_schema_similarity(self, schema1: Dict, schema2: Dict) -> float:
        """
        Ä°ki ÅŸema arasÄ±nda semantik benzerlik hesapla
        
        Returns:
            0.0 ile 1.0 arasÄ± similarity score
        """
        if not self.model:
            return 0.0
        
        # ÅemalarÄ± text'e Ã§evir
        text1 = self._schema_to_text(schema1)
        text2 = self._schema_to_text(schema2)
        
        # Embeddings hesapla
        embeddings = self.model.encode([text1, text2])
        
        # Cosine similarity
        from sklearn.metrics.pairwise import cosine_similarity
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        
        return float(similarity)
    
    def _schema_to_text(self, schema: Dict) -> str:
        """
        ÅemayÄ± text'e Ã§evir (embedding iÃ§in)
        
        TÃ¼m field'larÄ± anlamlÄ± bir text'e dÃ¶nÃ¼ÅŸtÃ¼r
        """
        output = schema.get('output', [])
        
        texts = []
        for entity in output:
            parts = []
            
            # Her field'Ä± ekle
            if entity.get('abnormality') and entity['abnormality'] != 'None':
                parts.append(f"abnormality: {entity['abnormality']}")
            
            if entity.get('presence') and entity['presence'] != 'None':
                parts.append(f"presence: {entity['presence']}")
            
            if entity.get('location'):
                loc = entity['location']
                if isinstance(loc, list):
                    parts.append(f"location: {', '.join(loc)}")
                else:
                    parts.append(f"location: {loc}")
            
            if entity.get('degree') and entity['degree'] != 'None':
                deg = entity['degree']
                if isinstance(deg, list):
                    parts.append(f"degree: {', '.join(deg)}")
                else:
                    parts.append(f"degree: {deg}")
            
            if entity.get('measurement') and entity['measurement'] != 'None':
                parts.append(f"measurement: {entity['measurement']}")
            
            if parts:
                texts.append(' | '.join(parts))
        
        return ' ; '.join(texts)


# ============================================================================
# KullanÄ±m Ã–rnekleri
# ============================================================================

def example_basic_usage():
    """
    Ã–RNEK 1: Temel kullanÄ±m (sizin kod yapÄ±nÄ±za uygun)
    """
    print("\n" + "="*70)
    print("Ã–RNEK 1: Temel LLM DeÄŸerlendirme")
    print("="*70)
    
    # API keylerini tanÄ±mla
    API_KEYS = {
        "gemini": "AIzaSyAVNB1TaFtlLSUZeUkB3n9C0-zn82ITLws",
        "gemma": "KGAT_7b8482384bb20717b1fa8b9c914ff365",
        "glm": "sk-t80kLqA1bkLIoTi0x0vjmno3-gbMvrX3A44SOh4QWHRpiYJvMeOTpUOScAAWzOPzpDxC8AyC0KPdgaqHrn_5RPa_RhY_",
        "deepseek": "sk-450186e490b34beb8347badc0fa91e6b",
    }


    
    # Evaluator oluÅŸtur (model seÃ§imi Ã§ok basit!)
    evaluator = LLMEvaluator(
        model_type="gemini",              # "gemini", "gemma", "glm", "deepseek"
        model_name="gemini-1.5-flash",    # Model adÄ±
        api_key=API_KEYS["gemini"]        # API key
    )
    
    # Ã–rnek ÅŸemalar
    ground_truth = {
        'input': 'Spleen size is enlarged and measured 134 mm.',
        'output': [{
            'abnormality': 'enlarged',
            'presence': 'present',
            'location': ['spleen'],
            'degree': 'None',
            'measurement': '134 mm',
            'comparison': 'None'
        }]
    }
    
    prediction = {
        'input': 'Spleen size is enlarged and measured 134 mm.',
        'output': [{
            'abnormality': 'splenomegaly',  # FarklÄ± kelime, aynÄ± anlam
            'presence': 'present',
            'location': ['spleen'],
            'degree': 'None',
            'measurement': '134mm',  # BoÅŸluk farkÄ±
            'comparison': 'None'
        }]
    }
    
    # DeÄŸerlendir!
    result = evaluator.evaluate_schema_pair(
        ground_truth,
        prediction,
        ground_truth['input']
    )
    
    # SonuÃ§larÄ± yazdÄ±r
    print(f"\nğŸ“Š SONUÃ‡LAR:")
    print(f"  Benzerlik skoru: {result.get('similarity_score', 0):.3f}")
    print(f"  Klinik eÅŸdeÄŸerlik: {result.get('clinical_equivalence', 'unknown')}")
    print(f"  AynÄ± anlama mÄ± geliyor: {result.get('are_same_meaning', False)}")
    print(f"\n  DeÄŸerlendirme: {result.get('overall_assessment', 'N/A')}")


def example_batch_evaluation():
    """
    Ã–RNEK 2: Toplu deÄŸerlendirme (sizin map_schema yapÄ±nÄ±z gibi)
    """
    print("\n" + "="*70)
    print("Ã–RNEK 2: Toplu DeÄŸerlendirme")
    print("="*70)
    
    # API key
    API_KEY = "AIzaSy..."  # Kendi keyiniz
    
    # Evaluator
    evaluator = LLMEvaluator(
        model_type="gemini",
        model_name="gemini-1.5-flash",
        api_key=API_KEY
    )
    
    # Ground truth ve predictions yÃ¼kle
    with open('schema_train.json', 'r') as f:
        ground_truths = json.load(f)
    
    with open('your_model_predictions.json', 'r') as f:
        predictions = json.load(f)
    
    # KarÅŸÄ±laÅŸtÄ±rma listesi hazÄ±rla
    comparisons = []
    for gt in ground_truths:
        # Matching prediction bul (input'a gÃ¶re)
        pred = next(
            (p for p in predictions if p['input'] == gt['input']),
            None
        )
        
        if pred:
            comparisons.append({
                'ground_truth': gt,
                'prediction': pred,
                'input': gt['input']
            })
    
    # Toplu deÄŸerlendirme (sizin map_schema gibi)
    results = evaluator.batch_evaluate(comparisons)
    
    # SonuÃ§larÄ± kaydet
    with open('llm_evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ {len(results)} deÄŸerlendirme tamamlandÄ±!")


def example_multi_model_comparison():
    """
    Ã–RNEK 3: Birden fazla model ile deÄŸerlendirme
    """
    print("\n" + "="*70)
    print("Ã–RNEK 3: Multi-Model KarÅŸÄ±laÅŸtÄ±rma")
    print("="*70)
    
    # TÃ¼m API keyler
    API_KEYS = {
        "gemini": "AIzaSyAVNB1TaFtlLSUZeUkB3n9C0-zn82ITLws",
        "gemma": "KGAT_7b8482384bb20717b1fa8b9c914ff365",
    }
    
    # Ã–rnek ÅŸema Ã§ifti
    gt = {...}
    pred = {...}
    
    # Her model ile deÄŸerlendir
    results = {}
    
    for model_type, api_key in API_KEYS.items():
        print(f"\nğŸ”„ {model_type} ile deÄŸerlendiriliyor...")
        
        evaluator = LLMEvaluator(
            model_type=model_type,
            model_name=f"{model_type}-1.5-flash",
            api_key=api_key
        )
        
        result = evaluator.evaluate_schema_pair(gt, pred, gt['input'])
        results[model_type] = result
    
    # KarÅŸÄ±laÅŸtÄ±r
    print(f"\nğŸ“Š MODEL KARÅILAÅTIRMASI:")
    for model, result in results.items():
        print(f"  {model:10s}: similarity={result['similarity_score']:.3f}")


if __name__ == '__main__':
    print("""

   pip install google-generativeai  # Gemini/Gemma iÃ§in
   pip install openai               # GLM/DeepSeek iÃ§in
   pip install sentence-transformers  # Embedding iÃ§in


   # Bu dosyayÄ± dÃ¼zenleyin ve Ã¶rnekleri test edin
   python llm_evaluator.py

    """)
    
    # Ã–rnekleri Ã§alÄ±ÅŸtÄ±rmak isterseniz uncomment edin:
    example_basic_usage()
    example_batch_evaluation()
    example_multi_model_comparison()

