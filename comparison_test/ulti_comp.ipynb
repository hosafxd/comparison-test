{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217dbb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting auto-normalization...\n",
      "\n",
      "======================================================================\n",
      "AUTO JSON NORMALIZATION\n",
      "======================================================================\n",
      "\n",
      "Base directory: data\n",
      "Found 10 data directories: ['1', '2', '3', '4', '5', '6', '0', '7', '8', '9']\n",
      "\n",
      "======================================================================\n",
      "Processing: data/0\n",
      "======================================================================\n",
      "Found 6 JSON files\n",
      "  ‚úÖ gt0.json ‚Üí 0_normalized/gt0.json\n",
      "  ‚úÖ sample0.0.json ‚Üí 0_normalized/sample0.0.json\n",
      "  ‚úÖ sample0.1.json ‚Üí 0_normalized/sample0.1.json\n",
      "  ‚úÖ sample0.2.json ‚Üí 0_normalized/sample0.2.json\n",
      "  ‚úÖ sample0.3.json ‚Üí 0_normalized/sample0.3.json\n",
      "  ‚úÖ sample0.4.json ‚Üí 0_normalized/sample0.4.json\n",
      "\n",
      "======================================================================\n",
      "Processing: data/1\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/1\n",
      "\n",
      "======================================================================\n",
      "Processing: data/2\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/2\n",
      "\n",
      "======================================================================\n",
      "Processing: data/3\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/3\n",
      "\n",
      "======================================================================\n",
      "Processing: data/4\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/4\n",
      "\n",
      "======================================================================\n",
      "Processing: data/5\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/5\n",
      "\n",
      "======================================================================\n",
      "Processing: data/6\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/6\n",
      "\n",
      "======================================================================\n",
      "Processing: data/7\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/7\n",
      "\n",
      "======================================================================\n",
      "Processing: data/8\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/8\n",
      "\n",
      "======================================================================\n",
      "Processing: data/9\n",
      "======================================================================\n",
      "‚ö† No JSON files found in data/9\n",
      "\n",
      "======================================================================\n",
      "NORMALIZATION SUMMARY\n",
      "======================================================================\n",
      "Total processed: 6\n",
      "Total errors: 0\n",
      "\n",
      "‚úÖ ALL FILES NORMALIZED SUCCESSFULLY!\n",
      "\n",
      "======================================================================\n",
      "‚úÖ READY FOR EVALUATION!\n",
      "======================================================================\n",
      "\n",
      "Normalized directories:\n",
      "  data/0_normalized/: 6 files\n",
      "  data/1_normalized/: 0 files\n",
      "  data/2_normalized/: 0 files\n",
      "  data/3_normalized/: 0 files\n",
      "  data/4_normalized/: 0 files\n",
      "  data/5_normalized/: 0 files\n",
      "  data/6_normalized/: 0 files\n",
      "  data/7_normalized/: 0 files\n",
      "  data/8_normalized/: 0 files\n",
      "  data/9_normalized/: 0 files\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: AUTO JSON NORMALIZATION & VALIDATION\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Bu cell:\n",
    "1. data/X/ klas√∂rlerindeki T√úM JSON dosyalarƒ±nƒ± bulur\n",
    "2. Formatƒ± kontrol eder\n",
    "3. Otomatik d√ºzeltir\n",
    "4. data/X_normalized/ klas√∂r√ºne kaydeder\n",
    "5. Hata raporu verir\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# ============================================================================\n",
    "# NORMALIZATION RULES\n",
    "# ============================================================================\n",
    "\n",
    "CANONICAL_ENTITY = {\n",
    "    \"abnormality\": None,\n",
    "    \"finding\": None,\n",
    "    \"presence\": \"unknown\",\n",
    "    \"location\": [],\n",
    "    \"degree\": [],\n",
    "    \"measurement\": None,\n",
    "    \"comparison\": None\n",
    "}\n",
    "\n",
    "def normalize_entity(entity: dict) -> dict:\n",
    "    \"\"\"Normalize a single entity\"\"\"\n",
    "    normalized = deepcopy(CANONICAL_ENTITY)\n",
    "    \n",
    "    for key in normalized:\n",
    "        if key not in entity:\n",
    "            continue\n",
    "        \n",
    "        value = entity[key]\n",
    "        \n",
    "        # \"None\" string ‚Üí None\n",
    "        if isinstance(value, str) and value.lower() == \"none\":\n",
    "            value = None\n",
    "        \n",
    "        # Empty string ‚Üí None\n",
    "        if isinstance(value, str) and value.strip() == \"\":\n",
    "            value = None\n",
    "        \n",
    "        # location & degree MUST be list\n",
    "        if key in (\"location\", \"degree\"):\n",
    "            if value is None:\n",
    "                value = []\n",
    "            elif isinstance(value, str):\n",
    "                value = [value] if value.strip() else []\n",
    "            elif not isinstance(value, list):\n",
    "                value = [str(value)]\n",
    "            # Remove \"None\" from lists\n",
    "            value = [v for v in value if str(v).lower() != \"none\"]\n",
    "        \n",
    "        # presence normalize\n",
    "        if key == \"presence\":\n",
    "            if value is None:\n",
    "                value = \"unknown\"\n",
    "            elif isinstance(value, str):\n",
    "                value = value.lower()\n",
    "                if value not in (\"present\", \"absent\", \"uncertain\"):\n",
    "                    value = \"unknown\"\n",
    "        \n",
    "        normalized[key] = value\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def normalize_json(data: dict) -> dict:\n",
    "    \"\"\"Normalize entire JSON structure\"\"\"\n",
    "    \n",
    "    # Handle array format [{\"instruction\": ...}]\n",
    "    if isinstance(data, list):\n",
    "        if len(data) > 0:\n",
    "            data = data[0]\n",
    "        else:\n",
    "            raise ValueError(\"Empty array\")\n",
    "    \n",
    "    # Ensure required fields\n",
    "    if \"output\" not in data:\n",
    "        raise ValueError(\"Missing 'output' field\")\n",
    "    \n",
    "    normalized_output = []\n",
    "    for ent in data.get(\"output\", []):\n",
    "        normalized_output.append(normalize_entity(ent))\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": data.get(\"instruction\", \"\").strip() or \n",
    "                      \"Extract medical entities from the given radiology report snippet and format them into the specified JSON schema. Pay attention to negations and normal anatomy.\",\n",
    "        \"input\": data.get(\"input\", \"\").strip(),\n",
    "        \"output\": normalized_output\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-DISCOVER & NORMALIZE ALL DIRECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "def auto_normalize_all(base_dir: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Automatically find and normalize all data directories\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"‚ùå Base directory not found: {base_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Find all numbered directories (0, 1, 2, ...)\n",
    "    data_dirs = [d for d in base_path.iterdir() \n",
    "                 if d.is_dir() and d.name.isdigit()]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"AUTO JSON NORMALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBase directory: {base_dir}\")\n",
    "    print(f\"Found {len(data_dirs)} data directories: {[d.name for d in data_dirs]}\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for data_dir in sorted(data_dirs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing: {data_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Create normalized output directory\n",
    "        output_dir = base_path / f\"{data_dir.name}_normalized\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Find all JSON files (gt*.json and sample*.json)\n",
    "        json_files = list(data_dir.glob(\"gt*.json\")) + list(data_dir.glob(\"sample*.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            print(f\"‚ö† No JSON files found in {data_dir}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "        \n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                # Load\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    raw_data = json.load(f)\n",
    "                \n",
    "                # Normalize\n",
    "                normalized = normalize_json(raw_data)\n",
    "                \n",
    "                # Save\n",
    "                output_file = output_dir / json_file.name\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(normalized, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                print(f\"  ‚úÖ {json_file.name} ‚Üí {output_dir.name}/{json_file.name}\")\n",
    "                total_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå {json_file.name}: {e}\")\n",
    "                total_errors += 1\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"NORMALIZATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total processed: {total_processed}\")\n",
    "    print(f\"Total errors: {total_errors}\")\n",
    "    \n",
    "    if total_errors == 0:\n",
    "        print(\"\\n‚úÖ ALL FILES NORMALIZED SUCCESSFULLY!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö† {total_errors} files had errors\")\n",
    "    \n",
    "    return total_processed, total_errors\n",
    "\n",
    "# ============================================================================\n",
    "# RUN AUTO-NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîÑ Starting auto-normalization...\\n\")\n",
    "processed, errors = auto_normalize_all(\"data\")\n",
    "\n",
    "if errors == 0 and processed > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ READY FOR EVALUATION!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNormalized directories:\")\n",
    "    for i in range(10):  # Check 0-9\n",
    "        norm_dir = Path(f\"data/{i}_normalized\")\n",
    "        if norm_dir.exists():\n",
    "            files = list(norm_dir.glob(\"*.json\"))\n",
    "            print(f\"  data/{i}_normalized/: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dcf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d5b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/Downloads/D√ñNEM6/MEDICAL_IMAGING/RaTEScore/venv310/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE MULTI-MODEL EVALUATOR\n",
      "======================================================================\n",
      "\n",
      "Data directory: data/0_normalized\n",
      "Output directory: data/0_normalized/ulti_comp_results\n",
      "\n",
      "LLM models to test: 1\n",
      "  - gemini_flash: models/gemini-2.5-flash\n",
      "\n",
      "Embedding models to test: 4\n",
      "  - general_baseline: sentence-transformers/all-MiniLM-L6-v2\n",
      "  - pubmedbert: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "  - s_pubmedbert: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "  - neuml_pubmedbert: NeuML/pubmedbert-base-embeddings\n",
      "\n",
      "‚ö†Ô∏è  Total combinations: 10\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "Ground truth: gt0.json\n",
      "Samples: 5\n",
      "\n",
      "======================================================================\n",
      "MODE 1: STRUCTURAL EVALUATION ONLY\n",
      "======================================================================\n",
      "  sample0.0.json: 0.540\n",
      "  sample0.1.json: 0.270\n",
      "  sample0.2.json: 0.500\n",
      "  sample0.3.json: 0.530\n",
      "  sample0.4.json: 0.902\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/structural_only/\n",
      "\n",
      "======================================================================\n",
      "MODE 2: EMBEDDING MODELS EVALUATION\n",
      "======================================================================\n",
      "\n",
      "--- Testing: general_baseline ---\n",
      "    Loading: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 680.08it/s, Materializing param=pooler.dense.weight]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "  sample0.0.json: struct=0.540, sem=0.786\n",
      "  sample0.1.json: struct=0.270, sem=0.226\n",
      "  sample0.2.json: struct=0.500, sem=0.919\n",
      "  sample0.3.json: struct=0.530, sem=0.681\n",
      "  sample0.4.json: struct=0.902, sem=0.926\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/embedding_general_baseline/\n",
      "\n",
      "--- Testing: pubmedbert ---\n",
      "    Loading: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with mean pooling.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 627.30it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "  sample0.0.json: struct=0.540, sem=0.984\n",
      "  sample0.1.json: struct=0.270, sem=0.966\n",
      "  sample0.2.json: struct=0.500, sem=0.992\n",
      "  sample0.3.json: struct=0.530, sem=0.995\n",
      "  sample0.4.json: struct=0.902, sem=0.995\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/embedding_pubmedbert/\n",
      "\n",
      "--- Testing: s_pubmedbert ---\n",
      "    Loading: pritamdeka/S-PubMedBert-MS-MARCO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 746.21it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "  sample0.0.json: struct=0.540, sem=0.970\n",
      "  sample0.1.json: struct=0.270, sem=0.896\n",
      "  sample0.2.json: struct=0.500, sem=0.969\n",
      "  sample0.3.json: struct=0.530, sem=0.975\n",
      "  sample0.4.json: struct=0.902, sem=0.988\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/embedding_s_pubmedbert/\n",
      "\n",
      "--- Testing: neuml_pubmedbert ---\n",
      "    Loading: NeuML/pubmedbert-base-embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 785.82it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "  sample0.0.json: struct=0.540, sem=0.791\n",
      "  sample0.1.json: struct=0.270, sem=0.056\n",
      "  sample0.2.json: struct=0.500, sem=0.888\n",
      "  sample0.3.json: struct=0.530, sem=0.818\n",
      "  sample0.4.json: struct=0.902, sem=0.964\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/embedding_neuml_pubmedbert/\n",
      "\n",
      "======================================================================\n",
      "MODE 3: LLM MODELS EVALUATION\n",
      "======================================================================\n",
      "\n",
      "--- Testing: gemini_flash ---\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-flash\n",
      "  Rate limit: Her istek arasƒ± 1.0 saniye bekleme\n",
      "  sample0.0.json: struct=0.540, llm=0.400\n",
      "  sample0.1.json: struct=0.270, llm=0.000\n",
      "  sample0.2.json: struct=0.500, llm=0.000\n",
      "  sample0.3.json: struct=0.530, llm=0.100\n",
      "  sample0.4.json: struct=0.902, llm=0.700\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/llm_gemini_flash/\n",
      "\n",
      "======================================================================\n",
      "MODE 4: COMBINED EVALUATION (Embedding + LLM)\n",
      "======================================================================\n",
      "Total combinations: 4\n",
      "\n",
      "[1/4] general_baseline + gemini_flash\n",
      "    Loading: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 946.12it/s, Materializing param=pooler.dense.weight]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-flash\n",
      "  Rate limit: Her istek arasƒ± 1.0 saniye bekleme\n",
      "  sample0.0.json: struct=0.540, sem=0.786, llm=0.350\n",
      "  sample0.1.json: struct=0.270, sem=0.226, llm=0.000\n",
      "  sample0.2.json: struct=0.500, sem=0.919, llm=0.000\n",
      "  sample0.3.json: struct=0.530, sem=0.681, llm=0.333\n",
      "  sample0.4.json: struct=0.902, sem=0.926, llm=0.900\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/combined_general_baseline_gemini_flash/\n",
      "\n",
      "[2/4] pubmedbert + gemini_flash\n",
      "    Loading: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with mean pooling.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 828.41it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-flash\n",
      "  Rate limit: Her istek arasƒ± 1.0 saniye bekleme\n",
      "  sample0.0.json: struct=0.540, sem=0.984, llm=0.400\n",
      "  sample0.1.json: struct=0.270, sem=0.966, llm=0.000\n",
      "  sample0.2.json: struct=0.500, sem=0.992, llm=0.000\n",
      "  sample0.3.json: struct=0.530, sem=0.995, llm=0.100\n",
      "  sample0.4.json: struct=0.902, sem=0.995, llm=0.920\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/combined_pubmedbert_gemini_flash/\n",
      "\n",
      "[3/4] s_pubmedbert + gemini_flash\n",
      "    Loading: pritamdeka/S-PubMedBert-MS-MARCO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 956.37it/s, Materializing param=pooler.dense.weight]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-flash\n",
      "  Rate limit: Her istek arasƒ± 1.0 saniye bekleme\n",
      "  sample0.0.json: struct=0.540, sem=0.970, llm=0.600\n",
      "  sample0.1.json: struct=0.270, sem=0.896, llm=0.000\n",
      "  sample0.2.json: struct=0.500, sem=0.969, llm=0.100\n",
      "  sample0.3.json: struct=0.530, sem=0.975, llm=0.100\n",
      "  sample0.4.json: struct=0.902, sem=0.988, llm=1.000\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/combined_s_pubmedbert_gemini_flash/\n",
      "\n",
      "[4/4] neuml_pubmedbert + gemini_flash\n",
      "    Loading: NeuML/pubmedbert-base-embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 833.49it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Loaded successfully\n",
      "‚úì gemini modeli ba≈ülatƒ±ldƒ±: models/gemini-2.5-flash\n",
      "  Rate limit: Her istek arasƒ± 1.0 saniye bekleme\n",
      "  sample0.0.json: struct=0.540, sem=0.791, llm=0.400\n",
      "  sample0.1.json: struct=0.270, sem=0.056, llm=0.000\n",
      "  sample0.2.json: struct=0.500, sem=0.888, llm=0.000\n",
      "  sample0.3.json: struct=0.530, sem=0.818, llm=0.390\n",
      "  sample0.4.json: struct=0.902, sem=0.964, llm=0.950\n",
      "    üíæ Saved to: data/0_normalized/ulti_comp_results/combined_neuml_pubmedbert_gemini_flash/\n",
      "\n",
      "======================================================================\n",
      "‚úÖ FINAL REPORT: data/0_normalized/ulti_comp_results/FINAL_COMPARISON_REPORT.txt\n",
      "======================================================================\n",
      "\n",
      "‚úÖ EVALUATION COMPLETE!\n",
      "Results saved to: ./data/0_normalized/ulti_comp_results//\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# comprehensive_evaluation.ipynb - CELL 1\n",
    "# ============================================================================\n",
    "\n",
    "from comprehensive_evaluation import ComprehensiveMultiModelEvaluator\n",
    "    \n",
    "# API Keys\n",
    "API_KEYS = {\n",
    "    \"gemini\": \"AIzaSyDKfk3iyWUilm8SU-f70PSRjo9etZBxrDk\",\n",
    "    \"gemma\": \"KGAT_7b8482384bb20717b1fa8b9c914ff365\",\n",
    "    \"glm\": \"sk-t80kLqA1bkLIoTi0x0vjmno3-gbMvrX3A44SOh4QWHRpiYJvMeOTpUOScAAWzOPzpDxC8AyC0KPdgaqHrn_5RPa_RhY_\",\n",
    "    \"deepseek\": \"sk-450186e490b34beb8347badc0fa91e6b\",\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"./data/0_normalized/\"\n",
    "OUTPUT_DIR = \"./data/0_normalized/ulti_comp_results/\"\n",
    "GT_FILE = \"gt0.json\"\n",
    "\n",
    "# Model selection (ba≈ülangƒ±√ß i√ßin az sayƒ±da)\n",
    "SELECTED_LLMS = [\n",
    "    \"gemini_flash\",      # Hƒ±zlƒ± test i√ßin\n",
    "    # \"gemini_pro\",      # Kalite i√ßin (uncomment)\n",
    "    # \"gemma\",           # A√ßƒ±k kaynak (uncomment)\n",
    "]\n",
    "\n",
    "SELECTED_EMBEDDINGS = [\n",
    "    \"general_baseline\",  # Baseline\n",
    "    \"pubmedbert\",     # PubMed (uncomment)\n",
    "    \"s_pubmedbert\",   # Clinical (uncomment)\n",
    "    \"neuml_pubmedbert\", # NeuML (uncomment)\n",
    "]\n",
    "\n",
    "\n",
    "# Run modes se√ß\n",
    "RUN_MODES = [\n",
    "    'structural',  # Baseline\n",
    "    'embedding',   # Sadece embedding\n",
    "    'llm',       # Sadece LLM (uncomment)\n",
    "    'combined'   # Full pipeline (uncomment - EN UZUN S√úRER)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize\n",
    "evaluator = ComprehensiveMultiModelEvaluator(\n",
    "    api_keys=API_KEYS,\n",
    "    data_dir=DATA_DIR,\n",
    "    output_base_dir=OUTPUT_DIR,\n",
    "    selected_llms=SELECTED_LLMS,\n",
    "    selected_embeddings=SELECTED_EMBEDDINGS\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RUN\n",
    "results = evaluator.run_full_evaluation(\n",
    "    gt_file=GT_FILE,\n",
    "    run_modes=RUN_MODES\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ EVALUATION COMPLETE!\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
