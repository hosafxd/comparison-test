{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e40173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SENTENCE-TRANSFORMERS DEBUG\n",
      "======================================================================\n",
      "\n",
      "âœ… sentence-transformers KURULU!\n",
      "   Version: 2.7.0\n",
      "   Location: /home/hosafxd/Downloads/DÃ–NEM6/MEDICAL_IMAGING/RaTEScore/venv310/lib64/python3.10/site-packages/sentence_transformers/__init__.py\n",
      "\n",
      "âœ… SentenceTransformer import edildi!\n",
      "\n",
      "ðŸ”„ Test model yÃ¼kleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/Downloads/DÃ–NEM6/MEDICAL_IMAGING/RaTEScore/venv310/lib64/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model baÅŸarÄ±yla yÃ¼klendi!\n",
      "âœ… Embedding hesaplandÄ±! Shape: (384,)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: SENTENCE-TRANSFORMERS DEBUG & FIX\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SENTENCE-TRANSFORMERS DEBUG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Kurulu mu kontrol et\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(f\"\\nâœ… sentence-transformers KURULU!\")\n",
    "    print(f\"   Version: {sentence_transformers.__version__}\")\n",
    "    print(f\"   Location: {sentence_transformers.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ sentence-transformers KURULU DEÄžÄ°L!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"\\nðŸ”§ Installing...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
    "    print(\"âœ… Kurulum tamamlandÄ± - KERNEL'I RESTART EDÄ°N!\")\n",
    "\n",
    "# 2. Test et\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"\\nâœ… SentenceTransformer import edildi!\")\n",
    "    \n",
    "    # Mini test\n",
    "    print(\"\\nðŸ”„ Test model yÃ¼kleniyor...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "    \n",
    "    # Test embedding\n",
    "    test_text = \"fracture of the distal radius\"\n",
    "    embedding = model.encode(test_text)\n",
    "    print(f\"âœ… Embedding hesaplandÄ±! Shape: {embedding.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Test FAILED: {e}\")\n",
    "    print(\"\\nâš ï¸ KERNEL'I RESTART EDÄ°N ve tekrar deneyin!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9952532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE MEDICAL SCHEMA EVALUATION - FULL FRAMEWORK\n",
    "# ============================================================================\n",
    "# Bu cell tÃ¼m framework'Ã¼ kullanÄ±r:\n",
    "# - medical_schema_evaluator.py (structural evaluation)\n",
    "# - llm_evaluator.py (LLM-based clinical validation)\n",
    "# - Statistical analysis\n",
    "# - Embedding-based semantic similarity\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Framework imports\n",
    "from medical_schema_evaluator import MedicalSchemaEvaluator, StatisticalAnalyzer\n",
    "from llm_evaluator import LLMEvaluator, EmbeddingBasedEvaluator\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION SECTION - CUSTOMIZE HERE!\n",
    "# ============================================================================\n",
    "\n",
    "class EvaluationConfig:\n",
    "    \"\"\"TÃ¼m ayarlar burada - kolayca deÄŸiÅŸtirilebilir\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = \"./data/0/\"\n",
    "    GT_FILE = \"gt0.json\"\n",
    "    OUTPUT_DIR = \"./data/0/results/\"\n",
    "    \n",
    "    # API Keys\n",
    "    API_KEYS = {\n",
    "            \"gemini\": \"AIzaSyAVNB1TaFtlLSUZeUkB3n9C0-zn82ITLws\",\n",
    "            \"gemma\": \"KGAT_7b8482384bb20717b1fa8b9c914ff365\",\n",
    "            \"glm\": \"sk-t80kLqA1bkLIoTi0x0vjmno3-gbMvrX3A44SOh4QWHRpiYJvMeOTpUOScAAWzOPzpDxC8AyC0KPdgaqHrn_5RPa_RhY_\",\n",
    "            \"deepseek\": \"sk-450186e490b34beb8347badc0fa91e6b\",\n",
    "        }\n",
    "    \n",
    "    # LLM Configuration\n",
    "    USE_LLM = True                    # LLM evaluation aÃ§Ä±k/kapalÄ±\n",
    "    LLM_MODEL_TYPE = \"gemini\"         # gemini, gemma, glm, deepseek\n",
    "    LLM_MODEL_NAME = \"gemini-2.5-pro\"  # Model adÄ±\n",
    "    \n",
    "    # Embedding Configuration\n",
    "    USE_EMBEDDINGS = True             # Semantic similarity aÃ§Ä±k/kapalÄ±\n",
    "    EMBEDDING_MODEL = \"pritamdeka/S-BioBERT-snli-multinli-stsb\"\n",
    "    \n",
    "    # Structural Evaluation Configuration\n",
    "    FIELD_WEIGHTS = {                 # CUSTOMIZE: Field importance weights\n",
    "        'abnormality': 0.30,          # ArtÄ±rÄ±ldÄ± - Ã§ok Ã¶nemli\n",
    "        'presence': 0.30,             # ArtÄ±rÄ±ldÄ± - kritik\n",
    "        'location': 0.20,\n",
    "        'degree': 0.10,\n",
    "        'measurement': 0.08,\n",
    "        'finding': 0.02,              # AzaltÄ±ldÄ±\n",
    "        'comparison': 0.00            # Genelde None\n",
    "    }\n",
    "    \n",
    "    # Analysis Options\n",
    "    ENTITY_MATCHING_THRESHOLD = 0.5   # Entity match iÃ§in min similarity\n",
    "    SAVE_INTERMEDIATE = True          # Her sample sonrasÄ± kaydet\n",
    "    VERBOSE = True                    # DetaylÄ± output\n",
    "    \n",
    "    # Statistical Analysis\n",
    "    BOOTSTRAP_ITERATIONS = 10000      # Bootstrap iÃ§in iterasyon sayÄ±sÄ±\n",
    "    CONFIDENCE_LEVEL = 0.95           # %95 confidence interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d4d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN EVALUATION CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ComprehensiveSchemaEvaluator:\n",
    "    \"\"\"\n",
    "    TÃ¼m evaluation metodlarÄ±nÄ± birleÅŸtiren ana class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        # Initialize evaluators\n",
    "        print(\"ðŸ”§ Initializing evaluators...\")\n",
    "        \n",
    "        # 1. Structural evaluator (ALWAYS used)\n",
    "        self.structural_eval = MedicalSchemaEvaluator()\n",
    "        # Customize field weights\n",
    "        self.structural_eval.FIELD_WEIGHTS = config.FIELD_WEIGHTS\n",
    "        print(\"  âœ“ Structural evaluator initialized\")\n",
    "        \n",
    "        # 2. LLM evaluator (OPTIONAL)\n",
    "        self.llm_eval = None\n",
    "        if config.USE_LLM:\n",
    "            try:\n",
    "                self.llm_eval = LLMEvaluator(\n",
    "                    model_type=config.LLM_MODEL_TYPE,\n",
    "                    model_name=config.LLM_MODEL_NAME,\n",
    "                    api_key=config.API_KEYS[config.LLM_MODEL_TYPE]\n",
    "                )\n",
    "                print(f\"  âœ“ LLM evaluator initialized ({config.LLM_MODEL_TYPE})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  LLM evaluator failed: {e}\")\n",
    "                config.USE_LLM = False\n",
    "        \n",
    "        # 3. Embedding evaluator (OPTIONAL)\n",
    "        self.embedding_eval = None\n",
    "        if config.USE_EMBEDDINGS:\n",
    "            try:\n",
    "                self.embedding_eval = EmbeddingBasedEvaluator(\n",
    "                    model_name=config.EMBEDDING_MODEL\n",
    "                )\n",
    "                print(f\"  âœ“ Embedding evaluator initialized\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Embedding evaluator failed: {e}\")\n",
    "                config.USE_EMBEDDINGS = False\n",
    "        \n",
    "        # 4. Statistical analyzer (ALWAYS available)\n",
    "        self.stat_analyzer = StatisticalAnalyzer()\n",
    "        print(\"  âœ“ Statistical analyzer initialized\")\n",
    "        \n",
    "        print(\"\\nâœ“ All evaluators ready!\\n\")\n",
    "    \n",
    "    def evaluate_single_pair(self, ground_truth, prediction, sample_name):\n",
    "        \"\"\"\n",
    "        Tek bir GT-prediction Ã§iftini evaluate et (TÃœM metodlarla)\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'sample_name': sample_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'input_text': ground_truth['input']\n",
    "        }\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 1. STRUCTURAL EVALUATION (Entity-by-entity comparison)\n",
    "        # ----------------------------------------------------------------\n",
    "        if self.config.VERBOSE:\n",
    "            print(f\"  [1/3] Structural evaluation...\")\n",
    "        \n",
    "        structural = self.structural_eval.compare_schemas(\n",
    "            ground_truth, \n",
    "            prediction\n",
    "        )\n",
    "        \n",
    "        result['structural'] = {\n",
    "            'overall_score': structural['overall_score'],\n",
    "            'num_gt_entities': structural['num_gt_entities'],\n",
    "            'num_pred_entities': structural['num_pred_entities'],\n",
    "            'entity_matches': structural['entity_matches'],\n",
    "            'field_scores': {\n",
    "                field: {\n",
    "                    'mean': float(np.mean(scores)) if scores else 0.0,\n",
    "                    'all_scores': scores\n",
    "                }\n",
    "                for field, scores in structural['field_scores'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.config.VERBOSE:\n",
    "            print(f\"    Overall score: {structural['overall_score']:.3f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 2. SEMANTIC EVALUATION (Embedding-based similarity)\n",
    "        # ----------------------------------------------------------------\n",
    "        if self.config.USE_EMBEDDINGS and self.embedding_eval:\n",
    "            if self.config.VERBOSE:\n",
    "                print(f\"  [2/3] Semantic similarity...\")\n",
    "            \n",
    "            semantic_sim = self.embedding_eval.compute_schema_similarity(\n",
    "                ground_truth,\n",
    "                prediction\n",
    "            )\n",
    "            result['semantic_similarity'] = float(semantic_sim)\n",
    "            \n",
    "            if self.config.VERBOSE:\n",
    "                print(f\"    Similarity: {semantic_sim:.3f}\")\n",
    "        else:\n",
    "            result['semantic_similarity'] = None\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 3. LLM EVALUATION (Clinical validation)\n",
    "        # ----------------------------------------------------------------\n",
    "        if self.config.USE_LLM and self.llm_eval:\n",
    "            if self.config.VERBOSE:\n",
    "                print(f\"  [3/3] LLM clinical validation...\")\n",
    "            \n",
    "            try:\n",
    "                llm_result = self.llm_eval.evaluate_schema_pair(\n",
    "                    ground_truth,\n",
    "                    prediction,\n",
    "                    ground_truth['input']\n",
    "                )\n",
    "                result['llm_evaluation'] = llm_result\n",
    "                \n",
    "                if self.config.VERBOSE:\n",
    "                    print(f\"    LLM score: {llm_result.get('similarity_score', 0):.3f}\")\n",
    "                    print(f\"    Equivalence: {llm_result.get('clinical_equivalence', 'unknown')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"    âš  LLM evaluation failed: {e}\")\n",
    "                result['llm_evaluation'] = {'error': str(e)}\n",
    "        else:\n",
    "            result['llm_evaluation'] = None\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 4. CUSTOM ANALYSIS - Entity matching details\n",
    "        # ----------------------------------------------------------------\n",
    "        result['entity_analysis'] = self._analyze_entity_matches(\n",
    "            ground_truth,\n",
    "            prediction,\n",
    "            structural['entity_matches']\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _analyze_entity_matches(self, gt, pred, matches):\n",
    "        \"\"\"\n",
    "        Entity matching'i detaylÄ± analiz et\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'perfect_matches': 0,\n",
    "            'partial_matches': 0,\n",
    "            'missing_entities': 0,\n",
    "            'extra_entities': 0,\n",
    "            'field_errors': defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for match in matches:\n",
    "            if match['matched_entity'] is None:\n",
    "                analysis['missing_entities'] += 1\n",
    "            elif match['similarity'] >= 0.95:\n",
    "                analysis['perfect_matches'] += 1\n",
    "            else:\n",
    "                analysis['partial_matches'] += 1\n",
    "        \n",
    "        # Extra entities (pred'de olup GT'de olmayan)\n",
    "        analysis['extra_entities'] = (\n",
    "            pred.get('output') and len(pred['output']) or 0\n",
    "        ) - len(matches)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def evaluate_directory(self, data_dir, gt_filename):\n",
    "        \"\"\"\n",
    "        Directory'deki tÃ¼m sample*.json dosyalarÄ±nÄ± evaluate et\n",
    "        \"\"\"\n",
    "        data_path = Path(data_dir)\n",
    "        gt_path = data_path / gt_filename\n",
    "        \n",
    "        # Load ground truth\n",
    "        with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truth = json.load(f)\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"GROUND TRUTH LOADED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"File: {gt_filename}\")\n",
    "        print(f\"Input: {ground_truth['input']}\")\n",
    "        print(f\"Entities: {len(ground_truth.get('output', []))}\")\n",
    "        \n",
    "        # Find all sample files\n",
    "        sample_files = sorted(data_path.glob(\"sample*.json\"))\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SAMPLES FOUND: {len(sample_files)}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for sf in sample_files:\n",
    "            print(f\"  - {sf.name}\")\n",
    "        \n",
    "        # Evaluate each sample\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"STARTING EVALUATION\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        for idx, sample_file in enumerate(sample_files):\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"[{idx+1}/{len(sample_files)}] {sample_file.name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Load sample\n",
    "            with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "                prediction = json.load(f)\n",
    "            \n",
    "            # Evaluate\n",
    "            result = self.evaluate_single_pair(\n",
    "                ground_truth,\n",
    "                prediction,\n",
    "                sample_file.name\n",
    "            )\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            # Print summary\n",
    "            self._print_result_summary(result)\n",
    "            \n",
    "            # Save intermediate (if enabled)\n",
    "            if self.config.SAVE_INTERMEDIATE:\n",
    "                self._save_results(suffix=f\"_after_{sample_file.stem}\")\n",
    "        \n",
    "        # Final analysis\n",
    "        self._final_analysis()\n",
    "        \n",
    "        # Save final results\n",
    "        self._save_results(suffix=\"_final\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _print_result_summary(self, result):\n",
    "        \"\"\"Print concise summary of single result\"\"\"\n",
    "        print(f\"\\nðŸ“Š SUMMARY:\")\n",
    "        print(f\"  Structural score: {result['structural']['overall_score']:.3f}\")\n",
    "        \n",
    "        if result['semantic_similarity']:\n",
    "            print(f\"  Semantic similarity: {result['semantic_similarity']:.3f}\")\n",
    "        \n",
    "        if result['llm_evaluation'] and 'similarity_score' in result['llm_evaluation']:\n",
    "            llm = result['llm_evaluation']\n",
    "            print(f\"  LLM score: {llm['similarity_score']:.3f}\")\n",
    "            print(f\"  LLM equivalence: {llm['clinical_equivalence']}\")\n",
    "        \n",
    "        # Entity analysis\n",
    "        ea = result['entity_analysis']\n",
    "        print(f\"\\n  Entity Analysis:\")\n",
    "        print(f\"    Perfect matches: {ea['perfect_matches']}\")\n",
    "        print(f\"    Partial matches: {ea['partial_matches']}\")\n",
    "        print(f\"    Missing: {ea['missing_entities']}\")\n",
    "        print(f\"    Extra: {ea['extra_entities']}\")\n",
    "    \n",
    "    def _final_analysis(self):\n",
    "        \"\"\"TÃ¼m sonuÃ§larÄ±n istatistiksel analizi\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FINAL STATISTICAL ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Structural scores\n",
    "        struct_scores = [r['structural']['overall_score'] for r in self.results]\n",
    "        \n",
    "        print(f\"\\nStructural Evaluation:\")\n",
    "        print(f\"  Mean: {np.mean(struct_scores):.4f}\")\n",
    "        print(f\"  Std:  {np.std(struct_scores):.4f}\")\n",
    "        print(f\"  Min:  {np.min(struct_scores):.4f}\")\n",
    "        print(f\"  Max:  {np.max(struct_scores):.4f}\")\n",
    "        \n",
    "        # Bootstrap CI\n",
    "        ci_lower, ci_upper = self.stat_analyzer.bootstrap_confidence_interval(\n",
    "            struct_scores,\n",
    "            n_bootstrap=self.config.BOOTSTRAP_ITERATIONS,\n",
    "            confidence=self.config.CONFIDENCE_LEVEL\n",
    "        )\n",
    "        print(f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "        \n",
    "        # Semantic scores (if available)\n",
    "        semantic_scores = [\n",
    "            r['semantic_similarity'] \n",
    "            for r in self.results \n",
    "            if r['semantic_similarity'] is not None\n",
    "        ]\n",
    "        \n",
    "        if semantic_scores:\n",
    "            print(f\"\\nSemantic Similarity:\")\n",
    "            print(f\"  Mean: {np.mean(semantic_scores):.4f}\")\n",
    "            print(f\"  Std:  {np.std(semantic_scores):.4f}\")\n",
    "        \n",
    "        # LLM scores (if available)\n",
    "        llm_scores = [\n",
    "            r['llm_evaluation']['similarity_score']\n",
    "            for r in self.results\n",
    "            if r['llm_evaluation'] and 'similarity_score' in r['llm_evaluation']\n",
    "        ]\n",
    "        \n",
    "        if llm_scores:\n",
    "            print(f\"\\nLLM Evaluation:\")\n",
    "            print(f\"  Mean: {np.mean(llm_scores):.4f}\")\n",
    "            print(f\"  Std:  {np.std(llm_scores):.4f}\")\n",
    "            \n",
    "            # Clinical equivalence distribution\n",
    "            equiv_dist = defaultdict(int)\n",
    "            for r in self.results:\n",
    "                if r['llm_evaluation'] and 'clinical_equivalence' in r['llm_evaluation']:\n",
    "                    equiv = r['llm_evaluation']['clinical_equivalence']\n",
    "                    equiv_dist[equiv] += 1\n",
    "            \n",
    "            print(f\"\\n  Clinical Equivalence Distribution:\")\n",
    "            for equiv, count in equiv_dist.items():\n",
    "                print(f\"    {equiv}: {count}\")\n",
    "        \n",
    "        # Field-wise analysis\n",
    "        print(f\"\\nField-wise Performance:\")\n",
    "        field_scores = defaultdict(list)\n",
    "        for r in self.results:\n",
    "            for field, data in r['structural']['field_scores'].items():\n",
    "                field_scores[field].append(data['mean'])\n",
    "        \n",
    "        for field in sorted(field_scores.keys()):\n",
    "            scores = field_scores[field]\n",
    "            weight = self.config.FIELD_WEIGHTS.get(field, 0)\n",
    "            print(f\"  {field:15s}: {np.mean(scores):.4f} (weight: {weight:.2f})\")\n",
    "        \n",
    "        # Entity matching summary\n",
    "        print(f\"\\nEntity Matching Summary:\")\n",
    "        total_perfect = sum(r['entity_analysis']['perfect_matches'] for r in self.results)\n",
    "        total_partial = sum(r['entity_analysis']['partial_matches'] for r in self.results)\n",
    "        total_missing = sum(r['entity_analysis']['missing_entities'] for r in self.results)\n",
    "        total_extra = sum(r['entity_analysis']['extra_entities'] for r in self.results)\n",
    "        \n",
    "        total_entities = sum(r['structural']['num_gt_entities'] for r in self.results)\n",
    "        \n",
    "        print(f\"  Perfect matches: {total_perfect}/{total_entities} ({total_perfect/total_entities*100:.1f}%)\")\n",
    "        print(f\"  Partial matches: {total_partial}/{total_entities} ({total_partial/total_entities*100:.1f}%)\")\n",
    "        print(f\"  Missing entities: {total_missing}/{total_entities} ({total_missing/total_entities*100:.1f}%)\")\n",
    "        print(f\"  Extra entities: {total_extra}\")\n",
    "    \n",
    "    def _save_results(self, suffix=\"\"):\n",
    "        \"\"\"Save results to JSON\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Detailed results\n",
    "        results_file = Path(self.config.OUTPUT_DIR) / f\"evaluation_results{suffix}.json\"\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Results saved: {results_file}\")\n",
    "        \n",
    "        # Summary report\n",
    "        summary_file = Path(self.config.OUTPUT_DIR) / f\"summary{suffix}.txt\"\n",
    "        self._write_summary_report(summary_file)\n",
    "        print(f\"ðŸ’¾ Summary saved: {summary_file}\")\n",
    "    \n",
    "    def _write_summary_report(self, output_path):\n",
    "        \"\"\"Write human-readable summary\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"MEDICAL SCHEMA EVALUATION - SUMMARY REPORT\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "            f.write(f\"Samples evaluated: {len(self.results)}\\n\")\n",
    "            f.write(f\"Ground truth: {self.config.GT_FILE}\\n\\n\")\n",
    "            \n",
    "            # Scores\n",
    "            struct_scores = [r['structural']['overall_score'] for r in self.results]\n",
    "            f.write(\"STRUCTURAL EVALUATION:\\n\")\n",
    "            f.write(f\"  Mean score: {np.mean(struct_scores):.4f}\\n\")\n",
    "            f.write(f\"  Std dev: {np.std(struct_scores):.4f}\\n\")\n",
    "            f.write(f\"  Range: [{np.min(struct_scores):.4f}, {np.max(struct_scores):.4f}]\\n\\n\")\n",
    "            \n",
    "            # Per-sample results\n",
    "            f.write(\"PER-SAMPLE RESULTS:\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            for r in self.results:\n",
    "                f.write(f\"{r['sample_name']:20s}: \")\n",
    "                f.write(f\"Structural={r['structural']['overall_score']:.3f}\")\n",
    "                if r['semantic_similarity']:\n",
    "                    f.write(f\", Semantic={r['semantic_similarity']:.3f}\")\n",
    "                if r['llm_evaluation'] and 'similarity_score' in r['llm_evaluation']:\n",
    "                    f.write(f\", LLM={r['llm_evaluation']['similarity_score']:.3f}\")\n",
    "                f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f0912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing evaluators...\n",
      "  âœ“ Structural evaluator initialized\n",
      "âœ“ gemini modeli baÅŸlatÄ±ldÄ±: gemini-2.5-pro\n",
      "  Rate limit: Her istek arasÄ± 1.0 saniye bekleme\n",
      "  âœ“ LLM evaluator initialized (gemini)\n",
      "âœ“ Embedding modeli yÃ¼klendi: pritamdeka/S-BioBERT-snli-multinli-stsb\n",
      "  âœ“ Embedding evaluator initialized\n",
      "  âœ“ Statistical analyzer initialized\n",
      "\n",
      "âœ“ All evaluators ready!\n",
      "\n",
      "======================================================================\n",
      "GROUND TRUTH LOADED\n",
      "======================================================================\n",
      "File: gt0.json\n",
      "Input: Acute displaced fracture of the distal radius with approximately 5 mm shortening.\n",
      "Entities: 1\n",
      "\n",
      "======================================================================\n",
      "SAMPLES FOUND: 5\n",
      "======================================================================\n",
      "  - sample0.0.json\n",
      "  - sample0.1.json\n",
      "  - sample0.2.json\n",
      "  - sample0.3.json\n",
      "  - sample0.4.json\n",
      "\n",
      "======================================================================\n",
      "STARTING EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[1/5] sample0.0.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.556\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.842\n",
      "  [3/3] LLM clinical validation...\n",
      "âš  JSON parse hatasÄ±: Unterminated string starting at: line 20 column 5 (char 672)\n",
      "Raw response (ilk 500 char): {\n",
      "  \"similarity_score\": 0.4,\n",
      "  \"clinical_equivalence\": \"low\",\n",
      "  \"are_same_meaning\": false,\n",
      "  \"entity_level_analysis\": [\n",
      "    {\n",
      "      \"gt_entity_idx\": 0,\n",
      "      \"pred_entity_idx\": 0,\n",
      "      \"field_scores\": {\n",
      "        \"abnormality\": 1.0,\n",
      "        \"presence\": 1.0,\n",
      "        \"location\": 1.0,\n",
      "        \"degree\": 0.67,\n",
      "        \"measurement\": 0.9\n",
      "      },\n",
      "      \"notes\": \"The prediction correctly identifies the location and presence of the fracture but introduces two clinically significant errors: it hallucinate\n",
      "    LLM score: 0.000\n",
      "    Equivalence: unknown\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.556\n",
      "  Semantic similarity: 0.842\n",
      "  LLM score: 0.000\n",
      "  LLM equivalence: unknown\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 0\n",
      "    Partial matches: 1\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.0.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.0.txt\n",
      "\n",
      "======================================================================\n",
      "[2/5] sample0.1.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.300\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.374\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 403 Your API key was reported as leaked. Please use another API key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 403 Your API key was reported as leaked. Please use another API key.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.300\n",
      "  Semantic similarity: 0.374\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 0\n",
      "    Partial matches: 1\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.1.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.1.txt\n",
      "\n",
      "======================================================================\n",
      "[3/5] sample0.2.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.520\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.726\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 403 Your API key was reported as leaked. Please use another API key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 403 Your API key was reported as leaked. Please use another API key.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.520\n",
      "  Semantic similarity: 0.726\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 1\n",
      "    Partial matches: 0\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.2.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.2.txt\n",
      "\n",
      "======================================================================\n",
      "[4/5] sample0.3.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.620\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.737\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 403 Your API key was reported as leaked. Please use another API key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 403 Your API key was reported as leaked. Please use another API key.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.620\n",
      "  Semantic similarity: 0.737\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 0\n",
      "    Partial matches: 1\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.3.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.3.txt\n",
      "\n",
      "======================================================================\n",
      "[5/5] sample0.4.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.931\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.983\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 403 Your API key was reported as leaked. Please use another API key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 403 Your API key was reported as leaked. Please use another API key.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.931\n",
      "  Semantic similarity: 0.983\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 1\n",
      "    Partial matches: 0\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.4.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.4.txt\n",
      "\n",
      "======================================================================\n",
      "FINAL STATISTICAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Structural Evaluation:\n",
      "  Mean: 0.5854\n",
      "  Std:  0.2036\n",
      "  Min:  0.3000\n",
      "  Max:  0.9310\n",
      "  95% CI: [0.4152, 0.7810]\n",
      "\n",
      "Semantic Similarity:\n",
      "  Mean: 0.7325\n",
      "  Std:  0.2017\n",
      "\n",
      "LLM Evaluation:\n",
      "  Mean: 0.0000\n",
      "  Std:  0.0000\n",
      "\n",
      "  Clinical Equivalence Distribution:\n",
      "    unknown: 1\n",
      "\n",
      "Field-wise Performance:\n",
      "  abnormality    : 0.6000 (weight: 0.30)\n",
      "  comparison     : 0.8000 (weight: 0.00)\n",
      "  degree         : 0.1900 (weight: 0.10)\n",
      "  finding        : 0.2000 (weight: 0.02)\n",
      "  location       : 0.6000 (weight: 0.20)\n",
      "  measurement    : 0.2800 (weight: 0.08)\n",
      "  presence       : 0.8000 (weight: 0.30)\n",
      "\n",
      "Entity Matching Summary:\n",
      "  Perfect matches: 2/5 (40.0%)\n",
      "  Partial matches: 3/5 (60.0%)\n",
      "  Missing entities: 0/5 (0.0%)\n",
      "  Extra entities: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_final.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_final.txt\n",
      "\n",
      "======================================================================\n",
      "BEST & WORST SAMPLES\n",
      "======================================================================\n",
      "\n",
      "Worst 2:\n",
      "  sample0.1.json: 0.300\n",
      "  sample0.2.json: 0.520\n",
      "\n",
      "Best 2:\n",
      "  sample0.3.json: 0.620\n",
      "  sample0.4.json: 0.931\n",
      "\n",
      "======================================================================\n",
      "FIELD-SPECIFIC ERRORS\n",
      "======================================================================\n",
      "\n",
      "ABNORMALITY:\n",
      "  sample0.0.json: 0.000\n",
      "  sample0.1.json: 0.000\n",
      "\n",
      "PRESENCE:\n",
      "  sample0.2.json: 0.000\n",
      "\n",
      "LOCATION:\n",
      "  sample0.1.json: 0.000\n",
      "  sample0.3.json: 0.000\n",
      "\n",
      "DEGREE:\n",
      "  sample0.0.json: 0.000\n",
      "  sample0.1.json: 0.000\n",
      "  sample0.2.json: 0.000\n",
      "  sample0.3.json: 0.200\n",
      "  sample0.4.json: 0.750\n",
      "\n",
      "MEASUREMENT:\n",
      "  sample0.0.json: 0.700\n",
      "  sample0.1.json: 0.000\n",
      "  sample0.2.json: 0.000\n",
      "  sample0.3.json: 0.000\n",
      "  sample0.4.json: 0.700\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize configuration\n",
    "config = EvaluationConfig()\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = ComprehensiveSchemaEvaluator(config)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_directory(\n",
    "    data_dir=config.DATA_DIR,\n",
    "    gt_filename=config.GT_FILE\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: CUSTOM ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Ã–rnek: En iyi ve en kÃ¶tÃ¼ Ã¶rnekleri bul\n",
    "struct_scores = [(r['sample_name'], r['structural']['overall_score']) for r in results]\n",
    "struct_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST & WORST SAMPLES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nWorst 2:\")\n",
    "for name, score in struct_scores[:2]:\n",
    "    print(f\"  {name}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nBest 2:\")\n",
    "for name, score in struct_scores[-2:]:\n",
    "    print(f\"  {name}: {score:.3f}\")\n",
    "\n",
    "# Ã–rnek: Field-specific error analysis\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FIELD-SPECIFIC ERRORS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for field_name in ['abnormality', 'presence', 'location', 'degree', 'measurement']:\n",
    "    print(f\"\\n{field_name.upper()}:\")\n",
    "    for r in results:\n",
    "        field_data = r['structural']['field_scores'].get(field_name, {})\n",
    "        mean_score = field_data.get('mean', 0)\n",
    "        if mean_score < 0.8:  # Show only problematic\n",
    "            print(f\"  {r['sample_name']}: {mean_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
