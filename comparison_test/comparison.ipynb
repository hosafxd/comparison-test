{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e590248d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gt.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1) DosyayÄ± oku\u001b[39;00m\n\u001b[1;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2) input -> index listesi eÅŸlemesi\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/DÃ–NEM6/MEDICAL_IMAGING/RaTEScore/venv310/lib64/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gt.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) DosyayÄ± oku\n",
    "file_path = \"gt.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2) input -> index listesi eÅŸlemesi\n",
    "input_map = defaultdict(list)\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    input_text = item.get(\"input\", \"\").strip()\n",
    "    input_map[input_text].append(idx)\n",
    "\n",
    "# 3) Duplicate olanlarÄ± filtrele\n",
    "duplicates = {\n",
    "    input_text: indices\n",
    "    for input_text, indices in input_map.items()\n",
    "    if len(indices) > 1\n",
    "}\n",
    "\n",
    "# 4) SonuÃ§larÄ± yazdÄ±r\n",
    "print(f\"Toplam duplicate input sayÄ±sÄ±: {len(duplicates)}\\n\")\n",
    "\n",
    "for i, (input_text, indices) in enumerate(duplicates.items(), 1):\n",
    "    print(f\"--- Duplicate #{i} ---\")\n",
    "    print(f\"Tekrar sayÄ±sÄ± : {len(indices)}\")\n",
    "    print(f\"Indexler      : {indices}\")\n",
    "    print(f\"Input         : {input_text}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d3bde44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "JSON FILE VALIDATION & FORMATTING\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Found 6 JSON files in data/0\n",
      "  - Ground truth files: 1\n",
      "  - Sample files: 5\n",
      "  - Other files: 0\n",
      "\n",
      "======================================================================\n",
      "VALIDATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Checking: sample0.4.json\n",
      "  âœ… VALID\n",
      "\n",
      "Checking: gt0.json\n",
      "  âœ… VALID\n",
      "\n",
      "Checking: sample0.3.json\n",
      "  âœ… VALID\n",
      "\n",
      "Checking: sample0.2.json\n",
      "  âœ… VALID\n",
      "\n",
      "Checking: sample0.1.json\n",
      "  âœ… VALID\n",
      "\n",
      "Checking: sample0.0.json\n",
      "  âœ… VALID\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Total files: 6\n",
      "Valid files: 6\n",
      "Fixed files: 0\n",
      "\n",
      "âœ… ALL FILES OK! Ready for evaluation.\n",
      "\n",
      "======================================================================\n",
      "FILE PREVIEWS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PREVIEW: gt0.json\n",
      "======================================================================\n",
      "Instruction: Extract medical entities from the given radiology report sni...\n",
      "Input: Acute displaced fracture of the distal radius with approximately 5 mm shortening...\n",
      "Entities: 1\n",
      "\n",
      "  Entity 0:\n",
      "    abnormality: fracture\n",
      "    presence: present\n",
      "    location: distal radius\n",
      "    measurement: 5 mm\n",
      "\n",
      "======================================================================\n",
      "PREVIEW: sample0.0.json\n",
      "======================================================================\n",
      "Instruction: Extract medical entities from the given radiology report sni...\n",
      "Input: Acute displaced fracture of the distal radius with approximately 5 mm shortening...\n",
      "Entities: 1\n",
      "\n",
      "  Entity 0:\n",
      "    abnormality: break\n",
      "    presence: present\n",
      "    location: distal radius\n",
      "    measurement: about 5 mm\n",
      "\n",
      "======================================================================\n",
      "DETAILED FORMAT CHECK\n",
      "======================================================================\n",
      "\n",
      "sample0.4.json:\n",
      "  Has 'instruction': âœ“\n",
      "  Has 'input': âœ“\n",
      "  Has 'output': âœ“\n",
      "  Output is list: âœ“\n",
      "  Number of entities: 1\n",
      "  First entity has all fields: âœ“\n",
      "\n",
      "gt0.json:\n",
      "  Has 'instruction': âœ“\n",
      "  Has 'input': âœ“\n",
      "  Has 'output': âœ“\n",
      "  Output is list: âœ“\n",
      "  Number of entities: 1\n",
      "  First entity has all fields: âœ“\n",
      "\n",
      "sample0.3.json:\n",
      "  Has 'instruction': âœ“\n",
      "  Has 'input': âœ“\n",
      "  Has 'output': âœ“\n",
      "  Output is list: âœ“\n",
      "  Number of entities: 1\n",
      "  First entity has all fields: âœ“\n",
      "\n",
      "sample0.2.json:\n",
      "  Has 'instruction': âœ“\n",
      "  Has 'input': âœ“\n",
      "  Has 'output': âœ“\n",
      "  Output is list: âœ“\n",
      "  Number of entities: 1\n",
      "  First entity has all fields: âœ“\n",
      "\n",
      "sample0.1.json:\n",
      "  Has 'instruction': âœ“\n",
      "  Has 'input': âœ“\n",
      "  Has 'output': âœ“\n",
      "  Output is list: âœ“\n",
      "  Number of entities: 1\n",
      "  First entity has all fields: âœ“\n",
      "\n",
      "sample0.0.json:\n",
      "  Has 'instruction': âœ“\n",
      "  Has 'input': âœ“\n",
      "  Has 'output': âœ“\n",
      "  Output is list: âœ“\n",
      "  Number of entities: 1\n",
      "  First entity has all fields: âœ“\n",
      "\n",
      "======================================================================\n",
      "âœ… ALL FILES VALIDATED AND READY!\n",
      "You can now run the evaluation pipeline.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# JSON FILE VALIDATOR & FORMATTER\n",
    "# ============================================================================\n",
    "# Bu cell:\n",
    "# 1. KlasÃ¶rdeki tÃ¼m JSON dosyalarÄ±nÄ± bulur\n",
    "# 2. Format kontrolÃ¼ yapar\n",
    "# 3. Gerekirse dÃ¼zeltir\n",
    "# 4. Validasyon raporu Ã§Ä±karÄ±r\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class SchemaValidator:\n",
    "    \"\"\"JSON schema dosyalarÄ±nÄ± validate ve dÃ¼zelt\"\"\"\n",
    "    \n",
    "    REQUIRED_FIELDS = ['instruction', 'input', 'output']\n",
    "    REQUIRED_ENTITY_FIELDS = [\n",
    "        'abnormality', 'finding', 'presence', \n",
    "        'location', 'degree', 'measurement', 'comparison'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        self.directory = Path(directory)\n",
    "        self.issues = []\n",
    "        self.fixed_files = []\n",
    "    \n",
    "    def validate_and_fix_all(self):\n",
    "        \"\"\"TÃ¼m JSON dosyalarÄ±nÄ± kontrol et ve dÃ¼zelt\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"JSON FILE VALIDATION & FORMATTING\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Find all JSON files\n",
    "        json_files = list(self.directory.glob(\"*.json\"))\n",
    "        \n",
    "        print(f\"\\nðŸ“ Found {len(json_files)} JSON files in {self.directory}\")\n",
    "        \n",
    "        # Separate GT and sample files\n",
    "        gt_files = [f for f in json_files if f.name.startswith('gt')]\n",
    "        sample_files = [f for f in json_files if f.name.startswith('sample')]\n",
    "        other_files = [f for f in json_files if f not in gt_files + sample_files]\n",
    "        \n",
    "        print(f\"  - Ground truth files: {len(gt_files)}\")\n",
    "        print(f\"  - Sample files: {len(sample_files)}\")\n",
    "        print(f\"  - Other files: {len(other_files)}\")\n",
    "        \n",
    "        # Validate each file\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"VALIDATION RESULTS\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        all_valid = True\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            print(f\"Checking: {json_file.name}\")\n",
    "            is_valid, issues, fixed_data = self.validate_file(json_file)\n",
    "            \n",
    "            if is_valid:\n",
    "                print(f\"  âœ… VALID\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  ISSUES FOUND:\")\n",
    "                for issue in issues:\n",
    "                    print(f\"      - {issue}\")\n",
    "                \n",
    "                # Try to fix\n",
    "                if fixed_data:\n",
    "                    self.save_fixed_file(json_file, fixed_data)\n",
    "                    print(f\"  âœ“ Auto-fixed and saved\")\n",
    "                    self.fixed_files.append(json_file.name)\n",
    "                else:\n",
    "                    print(f\"  âœ— Cannot auto-fix - manual intervention needed\")\n",
    "                    all_valid = False\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total files: {len(json_files)}\")\n",
    "        print(f\"Valid files: {len(json_files) - len(self.fixed_files)}\")\n",
    "        print(f\"Fixed files: {len(self.fixed_files)}\")\n",
    "        \n",
    "        if self.fixed_files:\n",
    "            print(f\"\\nFixed files:\")\n",
    "            for f in self.fixed_files:\n",
    "                print(f\"  - {f}\")\n",
    "        \n",
    "        if all_valid or self.fixed_files:\n",
    "            print(f\"\\nâœ… ALL FILES OK! Ready for evaluation.\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  Some files need manual fixing.\")\n",
    "        \n",
    "        return all_valid or len(self.fixed_files) > 0\n",
    "    \n",
    "    def validate_file(self, filepath):\n",
    "        \"\"\"Tek bir dosyayÄ± validate et\"\"\"\n",
    "        issues = []\n",
    "        fixed_data = None\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            issues.append(f\"Invalid JSON: {e}\")\n",
    "            return False, issues, None\n",
    "        except Exception as e:\n",
    "            issues.append(f\"Cannot read file: {e}\")\n",
    "            return False, issues, None\n",
    "        \n",
    "        # Check top-level structure\n",
    "        for field in self.REQUIRED_FIELDS:\n",
    "            if field not in data:\n",
    "                issues.append(f\"Missing required field: '{field}'\")\n",
    "        \n",
    "        # If missing top-level fields, try to fix\n",
    "        if issues and self._can_auto_fix_structure(data):\n",
    "            fixed_data = self._fix_structure(data)\n",
    "            return False, issues, fixed_data\n",
    "        \n",
    "        # Check 'output' is a list\n",
    "        if 'output' in data:\n",
    "            if not isinstance(data['output'], list):\n",
    "                issues.append(\"'output' should be a list\")\n",
    "                fixed_data = data.copy()\n",
    "                fixed_data['output'] = [data['output']]\n",
    "            \n",
    "            # Check each entity in output\n",
    "            for idx, entity in enumerate(data['output']):\n",
    "                if not isinstance(entity, dict):\n",
    "                    issues.append(f\"Entity {idx} is not a dictionary\")\n",
    "                    continue\n",
    "                \n",
    "                # Check required entity fields\n",
    "                for field in self.REQUIRED_ENTITY_FIELDS:\n",
    "                    if field not in entity:\n",
    "                        if not fixed_data:\n",
    "                            fixed_data = data.copy()\n",
    "                        # Add missing field with 'None'\n",
    "                        if 'output' not in fixed_data:\n",
    "                            fixed_data['output'] = []\n",
    "                        while len(fixed_data['output']) <= idx:\n",
    "                            fixed_data['output'].append({})\n",
    "                        fixed_data['output'][idx][field] = 'None'\n",
    "                        issues.append(f\"Entity {idx}: Missing field '{field}' - added as 'None'\")\n",
    "        \n",
    "        is_valid = len(issues) == 0\n",
    "        return is_valid, issues, fixed_data\n",
    "    \n",
    "    def _can_auto_fix_structure(self, data):\n",
    "        \"\"\"Otomatik dÃ¼zeltilebilir mi kontrol et\"\"\"\n",
    "        # EÄŸer data direkt olarak entity ise (output array eksikse)\n",
    "        if isinstance(data, dict) and any(\n",
    "            field in data for field in self.REQUIRED_ENTITY_FIELDS\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _fix_structure(self, data):\n",
    "        \"\"\"YapÄ±yÄ± dÃ¼zelt\"\"\"\n",
    "        fixed = {\n",
    "            'instruction': data.get(\n",
    "                'instruction',\n",
    "                'Extract medical entities from the given radiology report snippet and format them into the specified JSON schema. Pay attention to negations and normal anatomy.'\n",
    "            ),\n",
    "            'input': data.get('input', 'Unknown input'),\n",
    "            'output': []\n",
    "        }\n",
    "        \n",
    "        # EÄŸer data direkt entity ise, onu output'a ekle\n",
    "        if any(field in data for field in self.REQUIRED_ENTITY_FIELDS):\n",
    "            entity = {}\n",
    "            for field in self.REQUIRED_ENTITY_FIELDS:\n",
    "                entity[field] = data.get(field, 'None')\n",
    "            fixed['output'] = [entity]\n",
    "        elif 'output' in data:\n",
    "            fixed['output'] = data['output']\n",
    "        \n",
    "        return fixed\n",
    "    \n",
    "    def save_fixed_file(self, filepath, fixed_data):\n",
    "        \"\"\"DÃ¼zeltilmiÅŸ dosyayÄ± kaydet\"\"\"\n",
    "        # Backup original\n",
    "        backup_path = filepath.with_suffix('.json.bak')\n",
    "        if not backup_path.exists():\n",
    "            filepath.rename(backup_path)\n",
    "        \n",
    "        # Save fixed version\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(fixed_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def show_file_preview(self, filepath, max_entities=2):\n",
    "        \"\"\"Dosya iÃ§eriÄŸini Ã¶nizle\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PREVIEW: {filepath.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if 'instruction' in data:\n",
    "            print(f\"Instruction: {data['instruction'][:60]}...\")\n",
    "        \n",
    "        if 'input' in data:\n",
    "            print(f\"Input: {data['input'][:80]}...\")\n",
    "        \n",
    "        if 'output' in data:\n",
    "            print(f\"Entities: {len(data['output'])}\")\n",
    "            for idx, entity in enumerate(data['output'][:max_entities]):\n",
    "                print(f\"\\n  Entity {idx}:\")\n",
    "                for field in ['abnormality', 'presence', 'location', 'measurement']:\n",
    "                    if field in entity:\n",
    "                        val = entity[field]\n",
    "                        if isinstance(val, list):\n",
    "                            val = ', '.join(str(v) for v in val)\n",
    "                        print(f\"    {field}: {val}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "# Validate all files in directory\n",
    "validator = SchemaValidator(\"./data/0/\")\n",
    "all_ok = validator.validate_and_fix_all()\n",
    "\n",
    "# ============================================================================\n",
    "# PREVIEW FILES (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "if all_ok:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FILE PREVIEWS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Preview GT file\n",
    "    gt_files = list(Path(\"./data/0/\").glob(\"gt*.json\"))\n",
    "    if gt_files:\n",
    "        validator.show_file_preview(gt_files[0])\n",
    "    \n",
    "    # Preview first sample\n",
    "    sample_files = sorted(Path(\"./data/0/\").glob(\"sample*.json\"))\n",
    "    if sample_files:\n",
    "        validator.show_file_preview(sample_files[0])\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED FORMAT CHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DETAILED FORMAT CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "all_files = list(Path(\"./data/0/\").glob(\"*.json\"))\n",
    "for json_file in all_files:\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n{json_file.name}:\")\n",
    "    print(f\"  Has 'instruction': {'âœ“' if 'instruction' in data else 'âœ—'}\")\n",
    "    print(f\"  Has 'input': {'âœ“' if 'input' in data else 'âœ—'}\")\n",
    "    print(f\"  Has 'output': {'âœ“' if 'output' in data else 'âœ—'}\")\n",
    "    \n",
    "    if 'output' in data:\n",
    "        print(f\"  Output is list: {'âœ“' if isinstance(data['output'], list) else 'âœ—'}\")\n",
    "        print(f\"  Number of entities: {len(data['output'])}\")\n",
    "        \n",
    "        if data['output']:\n",
    "            entity = data['output'][0]\n",
    "            print(f\"  First entity has all fields: \", end='')\n",
    "            all_fields = all(\n",
    "                field in entity \n",
    "                for field in validator.REQUIRED_ENTITY_FIELDS\n",
    "            )\n",
    "            print('âœ“' if all_fields else 'âœ—')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if all_ok:\n",
    "    print(\"âœ… ALL FILES VALIDATED AND READY!\")\n",
    "    print(\"You can now run the evaluation pipeline.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Please fix the issues above before running evaluation.\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9952532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE MEDICAL SCHEMA EVALUATION - FULL FRAMEWORK\n",
    "# ============================================================================\n",
    "# Bu cell tÃ¼m framework'Ã¼ kullanÄ±r:\n",
    "# - medical_schema_evaluator.py (structural evaluation)\n",
    "# - llm_evaluator.py (LLM-based clinical validation)\n",
    "# - Statistical analysis\n",
    "# - Embedding-based semantic similarity\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Framework imports\n",
    "from medical_schema_evaluator import MedicalSchemaEvaluator, StatisticalAnalyzer\n",
    "from llm_evaluator import LLMEvaluator, EmbeddingBasedEvaluator\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION SECTION - CUSTOMIZE HERE!\n",
    "# ============================================================================\n",
    "\n",
    "class EvaluationConfig:\n",
    "    \"\"\"TÃ¼m ayarlar burada - kolayca deÄŸiÅŸtirilebilir\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = \"./data/0/\"\n",
    "    GT_FILE = \"gt0.json\"\n",
    "    OUTPUT_DIR = \"./data/0/results/\"\n",
    "    \n",
    "    # API Keys\n",
    "    API_KEYS = {\n",
    "            \"gemini\": \"AIzaSyAVNB1TaFtlLSUZeUkB3n9C0-zn82ITLws\",\n",
    "            \"gemma\": \"KGAT_7b8482384bb20717b1fa8b9c914ff365\",\n",
    "            \"glm\": \"sk-t80kLqA1bkLIoTi0x0vjmno3-gbMvrX3A44SOh4QWHRpiYJvMeOTpUOScAAWzOPzpDxC8AyC0KPdgaqHrn_5RPa_RhY_\",\n",
    "            \"deepseek\": \"sk-450186e490b34beb8347badc0fa91e6b\",\n",
    "        }\n",
    "    \n",
    "    # LLM Configuration\n",
    "    USE_LLM = True                    # LLM evaluation aÃ§Ä±k/kapalÄ±\n",
    "    LLM_MODEL_TYPE = \"gemini\"         # gemini, gemma, glm, deepseek\n",
    "    LLM_MODEL_NAME = \"gemini-1.5-flash\"  # Model adÄ±\n",
    "    \n",
    "    # Embedding Configuration\n",
    "    USE_EMBEDDINGS = True             # Semantic similarity aÃ§Ä±k/kapalÄ±\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    # Medikal domain iÃ§in: \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "    \n",
    "    # Structural Evaluation Configuration\n",
    "    FIELD_WEIGHTS = {                 # CUSTOMIZE: Field importance weights\n",
    "        'abnormality': 0.30,          # ArtÄ±rÄ±ldÄ± - Ã§ok Ã¶nemli\n",
    "        'presence': 0.30,             # ArtÄ±rÄ±ldÄ± - kritik\n",
    "        'location': 0.20,\n",
    "        'degree': 0.10,\n",
    "        'measurement': 0.08,\n",
    "        'finding': 0.02,              # AzaltÄ±ldÄ±\n",
    "        'comparison': 0.00            # Genelde None\n",
    "    }\n",
    "    \n",
    "    # Analysis Options\n",
    "    ENTITY_MATCHING_THRESHOLD = 0.5   # Entity match iÃ§in min similarity\n",
    "    SAVE_INTERMEDIATE = True          # Her sample sonrasÄ± kaydet\n",
    "    VERBOSE = True                    # DetaylÄ± output\n",
    "    \n",
    "    # Statistical Analysis\n",
    "    BOOTSTRAP_ITERATIONS = 10000      # Bootstrap iÃ§in iterasyon sayÄ±sÄ±\n",
    "    CONFIDENCE_LEVEL = 0.95           # %95 confidence interval\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EVALUATION CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ComprehensiveSchemaEvaluator:\n",
    "    \"\"\"\n",
    "    TÃ¼m evaluation metodlarÄ±nÄ± birleÅŸtiren ana class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        # Initialize evaluators\n",
    "        print(\"ðŸ”§ Initializing evaluators...\")\n",
    "        \n",
    "        # 1. Structural evaluator (ALWAYS used)\n",
    "        self.structural_eval = MedicalSchemaEvaluator()\n",
    "        # Customize field weights\n",
    "        self.structural_eval.FIELD_WEIGHTS = config.FIELD_WEIGHTS\n",
    "        print(\"  âœ“ Structural evaluator initialized\")\n",
    "        \n",
    "        # 2. LLM evaluator (OPTIONAL)\n",
    "        self.llm_eval = None\n",
    "        if config.USE_LLM:\n",
    "            try:\n",
    "                self.llm_eval = LLMEvaluator(\n",
    "                    model_type=config.LLM_MODEL_TYPE,\n",
    "                    model_name=config.LLM_MODEL_NAME,\n",
    "                    api_key=config.API_KEYS[config.LLM_MODEL_TYPE]\n",
    "                )\n",
    "                print(f\"  âœ“ LLM evaluator initialized ({config.LLM_MODEL_TYPE})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  LLM evaluator failed: {e}\")\n",
    "                config.USE_LLM = False\n",
    "        \n",
    "        # 3. Embedding evaluator (OPTIONAL)\n",
    "        self.embedding_eval = None\n",
    "        if config.USE_EMBEDDINGS:\n",
    "            try:\n",
    "                self.embedding_eval = EmbeddingBasedEvaluator(\n",
    "                    model_name=config.EMBEDDING_MODEL\n",
    "                )\n",
    "                print(f\"  âœ“ Embedding evaluator initialized\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Embedding evaluator failed: {e}\")\n",
    "                config.USE_EMBEDDINGS = False\n",
    "        \n",
    "        # 4. Statistical analyzer (ALWAYS available)\n",
    "        self.stat_analyzer = StatisticalAnalyzer()\n",
    "        print(\"  âœ“ Statistical analyzer initialized\")\n",
    "        \n",
    "        print(\"\\nâœ“ All evaluators ready!\\n\")\n",
    "    \n",
    "    def evaluate_single_pair(self, ground_truth, prediction, sample_name):\n",
    "        \"\"\"\n",
    "        Tek bir GT-prediction Ã§iftini evaluate et (TÃœM metodlarla)\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'sample_name': sample_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'input_text': ground_truth['input']\n",
    "        }\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 1. STRUCTURAL EVALUATION (Entity-by-entity comparison)\n",
    "        # ----------------------------------------------------------------\n",
    "        if self.config.VERBOSE:\n",
    "            print(f\"  [1/3] Structural evaluation...\")\n",
    "        \n",
    "        structural = self.structural_eval.compare_schemas(\n",
    "            ground_truth, \n",
    "            prediction\n",
    "        )\n",
    "        \n",
    "        result['structural'] = {\n",
    "            'overall_score': structural['overall_score'],\n",
    "            'num_gt_entities': structural['num_gt_entities'],\n",
    "            'num_pred_entities': structural['num_pred_entities'],\n",
    "            'entity_matches': structural['entity_matches'],\n",
    "            'field_scores': {\n",
    "                field: {\n",
    "                    'mean': float(np.mean(scores)) if scores else 0.0,\n",
    "                    'all_scores': scores\n",
    "                }\n",
    "                for field, scores in structural['field_scores'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.config.VERBOSE:\n",
    "            print(f\"    Overall score: {structural['overall_score']:.3f}\")\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 2. SEMANTIC EVALUATION (Embedding-based similarity)\n",
    "        # ----------------------------------------------------------------\n",
    "        if self.config.USE_EMBEDDINGS and self.embedding_eval:\n",
    "            if self.config.VERBOSE:\n",
    "                print(f\"  [2/3] Semantic similarity...\")\n",
    "            \n",
    "            semantic_sim = self.embedding_eval.compute_schema_similarity(\n",
    "                ground_truth,\n",
    "                prediction\n",
    "            )\n",
    "            result['semantic_similarity'] = float(semantic_sim)\n",
    "            \n",
    "            if self.config.VERBOSE:\n",
    "                print(f\"    Similarity: {semantic_sim:.3f}\")\n",
    "        else:\n",
    "            result['semantic_similarity'] = None\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 3. LLM EVALUATION (Clinical validation)\n",
    "        # ----------------------------------------------------------------\n",
    "        if self.config.USE_LLM and self.llm_eval:\n",
    "            if self.config.VERBOSE:\n",
    "                print(f\"  [3/3] LLM clinical validation...\")\n",
    "            \n",
    "            try:\n",
    "                llm_result = self.llm_eval.evaluate_schema_pair(\n",
    "                    ground_truth,\n",
    "                    prediction,\n",
    "                    ground_truth['input']\n",
    "                )\n",
    "                result['llm_evaluation'] = llm_result\n",
    "                \n",
    "                if self.config.VERBOSE:\n",
    "                    print(f\"    LLM score: {llm_result.get('similarity_score', 0):.3f}\")\n",
    "                    print(f\"    Equivalence: {llm_result.get('clinical_equivalence', 'unknown')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"    âš  LLM evaluation failed: {e}\")\n",
    "                result['llm_evaluation'] = {'error': str(e)}\n",
    "        else:\n",
    "            result['llm_evaluation'] = None\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # 4. CUSTOM ANALYSIS - Entity matching details\n",
    "        # ----------------------------------------------------------------\n",
    "        result['entity_analysis'] = self._analyze_entity_matches(\n",
    "            ground_truth,\n",
    "            prediction,\n",
    "            structural['entity_matches']\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _analyze_entity_matches(self, gt, pred, matches):\n",
    "        \"\"\"\n",
    "        Entity matching'i detaylÄ± analiz et\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            'perfect_matches': 0,\n",
    "            'partial_matches': 0,\n",
    "            'missing_entities': 0,\n",
    "            'extra_entities': 0,\n",
    "            'field_errors': defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for match in matches:\n",
    "            if match['matched_entity'] is None:\n",
    "                analysis['missing_entities'] += 1\n",
    "            elif match['similarity'] >= 0.95:\n",
    "                analysis['perfect_matches'] += 1\n",
    "            else:\n",
    "                analysis['partial_matches'] += 1\n",
    "        \n",
    "        # Extra entities (pred'de olup GT'de olmayan)\n",
    "        analysis['extra_entities'] = (\n",
    "            pred.get('output') and len(pred['output']) or 0\n",
    "        ) - len(matches)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def evaluate_directory(self, data_dir, gt_filename):\n",
    "        \"\"\"\n",
    "        Directory'deki tÃ¼m sample*.json dosyalarÄ±nÄ± evaluate et\n",
    "        \"\"\"\n",
    "        data_path = Path(data_dir)\n",
    "        gt_path = data_path / gt_filename\n",
    "        \n",
    "        # Load ground truth\n",
    "        with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truth = json.load(f)\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"GROUND TRUTH LOADED\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"File: {gt_filename}\")\n",
    "        print(f\"Input: {ground_truth['input']}\")\n",
    "        print(f\"Entities: {len(ground_truth.get('output', []))}\")\n",
    "        \n",
    "        # Find all sample files\n",
    "        sample_files = sorted(data_path.glob(\"sample*.json\"))\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SAMPLES FOUND: {len(sample_files)}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for sf in sample_files:\n",
    "            print(f\"  - {sf.name}\")\n",
    "        \n",
    "        # Evaluate each sample\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"STARTING EVALUATION\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        for idx, sample_file in enumerate(sample_files):\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"[{idx+1}/{len(sample_files)}] {sample_file.name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Load sample\n",
    "            with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "                prediction = json.load(f)\n",
    "            \n",
    "            # Evaluate\n",
    "            result = self.evaluate_single_pair(\n",
    "                ground_truth,\n",
    "                prediction,\n",
    "                sample_file.name\n",
    "            )\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            # Print summary\n",
    "            self._print_result_summary(result)\n",
    "            \n",
    "            # Save intermediate (if enabled)\n",
    "            if self.config.SAVE_INTERMEDIATE:\n",
    "                self._save_results(suffix=f\"_after_{sample_file.stem}\")\n",
    "        \n",
    "        # Final analysis\n",
    "        self._final_analysis()\n",
    "        \n",
    "        # Save final results\n",
    "        self._save_results(suffix=\"_final\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _print_result_summary(self, result):\n",
    "        \"\"\"Print concise summary of single result\"\"\"\n",
    "        print(f\"\\nðŸ“Š SUMMARY:\")\n",
    "        print(f\"  Structural score: {result['structural']['overall_score']:.3f}\")\n",
    "        \n",
    "        if result['semantic_similarity']:\n",
    "            print(f\"  Semantic similarity: {result['semantic_similarity']:.3f}\")\n",
    "        \n",
    "        if result['llm_evaluation'] and 'similarity_score' in result['llm_evaluation']:\n",
    "            llm = result['llm_evaluation']\n",
    "            print(f\"  LLM score: {llm['similarity_score']:.3f}\")\n",
    "            print(f\"  LLM equivalence: {llm['clinical_equivalence']}\")\n",
    "        \n",
    "        # Entity analysis\n",
    "        ea = result['entity_analysis']\n",
    "        print(f\"\\n  Entity Analysis:\")\n",
    "        print(f\"    Perfect matches: {ea['perfect_matches']}\")\n",
    "        print(f\"    Partial matches: {ea['partial_matches']}\")\n",
    "        print(f\"    Missing: {ea['missing_entities']}\")\n",
    "        print(f\"    Extra: {ea['extra_entities']}\")\n",
    "    \n",
    "    def _final_analysis(self):\n",
    "        \"\"\"TÃ¼m sonuÃ§larÄ±n istatistiksel analizi\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FINAL STATISTICAL ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Structural scores\n",
    "        struct_scores = [r['structural']['overall_score'] for r in self.results]\n",
    "        \n",
    "        print(f\"\\nStructural Evaluation:\")\n",
    "        print(f\"  Mean: {np.mean(struct_scores):.4f}\")\n",
    "        print(f\"  Std:  {np.std(struct_scores):.4f}\")\n",
    "        print(f\"  Min:  {np.min(struct_scores):.4f}\")\n",
    "        print(f\"  Max:  {np.max(struct_scores):.4f}\")\n",
    "        \n",
    "        # Bootstrap CI\n",
    "        ci_lower, ci_upper = self.stat_analyzer.bootstrap_confidence_interval(\n",
    "            struct_scores,\n",
    "            n_bootstrap=self.config.BOOTSTRAP_ITERATIONS,\n",
    "            confidence=self.config.CONFIDENCE_LEVEL\n",
    "        )\n",
    "        print(f\"  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "        \n",
    "        # Semantic scores (if available)\n",
    "        semantic_scores = [\n",
    "            r['semantic_similarity'] \n",
    "            for r in self.results \n",
    "            if r['semantic_similarity'] is not None\n",
    "        ]\n",
    "        \n",
    "        if semantic_scores:\n",
    "            print(f\"\\nSemantic Similarity:\")\n",
    "            print(f\"  Mean: {np.mean(semantic_scores):.4f}\")\n",
    "            print(f\"  Std:  {np.std(semantic_scores):.4f}\")\n",
    "        \n",
    "        # LLM scores (if available)\n",
    "        llm_scores = [\n",
    "            r['llm_evaluation']['similarity_score']\n",
    "            for r in self.results\n",
    "            if r['llm_evaluation'] and 'similarity_score' in r['llm_evaluation']\n",
    "        ]\n",
    "        \n",
    "        if llm_scores:\n",
    "            print(f\"\\nLLM Evaluation:\")\n",
    "            print(f\"  Mean: {np.mean(llm_scores):.4f}\")\n",
    "            print(f\"  Std:  {np.std(llm_scores):.4f}\")\n",
    "            \n",
    "            # Clinical equivalence distribution\n",
    "            equiv_dist = defaultdict(int)\n",
    "            for r in self.results:\n",
    "                if r['llm_evaluation'] and 'clinical_equivalence' in r['llm_evaluation']:\n",
    "                    equiv = r['llm_evaluation']['clinical_equivalence']\n",
    "                    equiv_dist[equiv] += 1\n",
    "            \n",
    "            print(f\"\\n  Clinical Equivalence Distribution:\")\n",
    "            for equiv, count in equiv_dist.items():\n",
    "                print(f\"    {equiv}: {count}\")\n",
    "        \n",
    "        # Field-wise analysis\n",
    "        print(f\"\\nField-wise Performance:\")\n",
    "        field_scores = defaultdict(list)\n",
    "        for r in self.results:\n",
    "            for field, data in r['structural']['field_scores'].items():\n",
    "                field_scores[field].append(data['mean'])\n",
    "        \n",
    "        for field in sorted(field_scores.keys()):\n",
    "            scores = field_scores[field]\n",
    "            weight = self.config.FIELD_WEIGHTS.get(field, 0)\n",
    "            print(f\"  {field:15s}: {np.mean(scores):.4f} (weight: {weight:.2f})\")\n",
    "        \n",
    "        # Entity matching summary\n",
    "        print(f\"\\nEntity Matching Summary:\")\n",
    "        total_perfect = sum(r['entity_analysis']['perfect_matches'] for r in self.results)\n",
    "        total_partial = sum(r['entity_analysis']['partial_matches'] for r in self.results)\n",
    "        total_missing = sum(r['entity_analysis']['missing_entities'] for r in self.results)\n",
    "        total_extra = sum(r['entity_analysis']['extra_entities'] for r in self.results)\n",
    "        \n",
    "        total_entities = sum(r['structural']['num_gt_entities'] for r in self.results)\n",
    "        \n",
    "        print(f\"  Perfect matches: {total_perfect}/{total_entities} ({total_perfect/total_entities*100:.1f}%)\")\n",
    "        print(f\"  Partial matches: {total_partial}/{total_entities} ({total_partial/total_entities*100:.1f}%)\")\n",
    "        print(f\"  Missing entities: {total_missing}/{total_entities} ({total_missing/total_entities*100:.1f}%)\")\n",
    "        print(f\"  Extra entities: {total_extra}\")\n",
    "    \n",
    "    def _save_results(self, suffix=\"\"):\n",
    "        \"\"\"Save results to JSON\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Detailed results\n",
    "        results_file = Path(self.config.OUTPUT_DIR) / f\"evaluation_results{suffix}.json\"\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Results saved: {results_file}\")\n",
    "        \n",
    "        # Summary report\n",
    "        summary_file = Path(self.config.OUTPUT_DIR) / f\"summary{suffix}.txt\"\n",
    "        self._write_summary_report(summary_file)\n",
    "        print(f\"ðŸ’¾ Summary saved: {summary_file}\")\n",
    "    \n",
    "    def _write_summary_report(self, output_path):\n",
    "        \"\"\"Write human-readable summary\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"MEDICAL SCHEMA EVALUATION - SUMMARY REPORT\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "            f.write(f\"Samples evaluated: {len(self.results)}\\n\")\n",
    "            f.write(f\"Ground truth: {self.config.GT_FILE}\\n\\n\")\n",
    "            \n",
    "            # Scores\n",
    "            struct_scores = [r['structural']['overall_score'] for r in self.results]\n",
    "            f.write(\"STRUCTURAL EVALUATION:\\n\")\n",
    "            f.write(f\"  Mean score: {np.mean(struct_scores):.4f}\\n\")\n",
    "            f.write(f\"  Std dev: {np.std(struct_scores):.4f}\\n\")\n",
    "            f.write(f\"  Range: [{np.min(struct_scores):.4f}, {np.max(struct_scores):.4f}]\\n\\n\")\n",
    "            \n",
    "            # Per-sample results\n",
    "            f.write(\"PER-SAMPLE RESULTS:\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            for r in self.results:\n",
    "                f.write(f\"{r['sample_name']:20s}: \")\n",
    "                f.write(f\"Structural={r['structural']['overall_score']:.3f}\")\n",
    "                if r['semantic_similarity']:\n",
    "                    f.write(f\", Semantic={r['semantic_similarity']:.3f}\")\n",
    "                if r['llm_evaluation'] and 'similarity_score' in r['llm_evaluation']:\n",
    "                    f.write(f\", LLM={r['llm_evaluation']['similarity_score']:.3f}\")\n",
    "                f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55f0912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing evaluators...\n",
      "  âœ“ Structural evaluator initialized\n",
      "âœ“ gemini modeli baÅŸlatÄ±ldÄ±: gemini-1.5-flash\n",
      "  Rate limit: Her istek arasÄ± 4.0 saniye bekleme\n",
      "  âœ“ LLM evaluator initialized (gemini)\n",
      "âš  sentence-transformers kurulu deÄŸil!\n",
      "  pip install sentence-transformers\n",
      "  âœ“ Embedding evaluator initialized\n",
      "  âœ“ Statistical analyzer initialized\n",
      "\n",
      "âœ“ All evaluators ready!\n",
      "\n",
      "======================================================================\n",
      "GROUND TRUTH LOADED\n",
      "======================================================================\n",
      "File: gt0.json\n",
      "Input: Acute displaced fracture of the distal radius with approximately 5 mm shortening.\n",
      "Entities: 1\n",
      "\n",
      "======================================================================\n",
      "SAMPLES FOUND: 5\n",
      "======================================================================\n",
      "  - sample0.0.json\n",
      "  - sample0.1.json\n",
      "  - sample0.2.json\n",
      "  - sample0.3.json\n",
      "  - sample0.4.json\n",
      "\n",
      "======================================================================\n",
      "STARTING EVALUATION\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[1/5] sample0.0.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.556\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.000\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.556\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 0\n",
      "    Partial matches: 1\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.0.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.0.txt\n",
      "\n",
      "======================================================================\n",
      "[2/5] sample0.1.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.300\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.000\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "LLM generation hatasÄ±: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.300\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 0\n",
      "    Partial matches: 1\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.1.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.1.txt\n",
      "\n",
      "======================================================================\n",
      "[3/5] sample0.2.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.520\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.000\n",
      "  [3/3] LLM clinical validation...\n",
      "    âš  LLM evaluation failed: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.520\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 1\n",
      "    Partial matches: 0\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.2.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.2.txt\n",
      "\n",
      "======================================================================\n",
      "[4/5] sample0.3.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.620\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.000\n",
      "  [3/3] LLM clinical validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM generation hatasÄ±: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "LLM generation hatasÄ±: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    âš  LLM evaluation failed: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.620\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 0\n",
      "    Partial matches: 1\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.3.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.3.txt\n",
      "\n",
      "======================================================================\n",
      "[5/5] sample0.4.json\n",
      "======================================================================\n",
      "  [1/3] Structural evaluation...\n",
      "    Overall score: 0.931\n",
      "  [2/3] Semantic similarity...\n",
      "    Similarity: 0.000\n",
      "  [3/3] LLM clinical validation...\n",
      "    âš  LLM evaluation failed: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "  Structural score: 0.931\n",
      "\n",
      "  Entity Analysis:\n",
      "    Perfect matches: 1\n",
      "    Partial matches: 0\n",
      "    Missing: 0\n",
      "    Extra: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_after_sample0.4.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_after_sample0.4.txt\n",
      "\n",
      "======================================================================\n",
      "FINAL STATISTICAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Structural Evaluation:\n",
      "  Mean: 0.5854\n",
      "  Std:  0.2036\n",
      "  Min:  0.3000\n",
      "  Max:  0.9310\n",
      "  95% CI: [0.4152, 0.7810]\n",
      "\n",
      "Semantic Similarity:\n",
      "  Mean: 0.0000\n",
      "  Std:  0.0000\n",
      "\n",
      "Field-wise Performance:\n",
      "  abnormality    : 0.6000 (weight: 0.30)\n",
      "  comparison     : 0.8000 (weight: 0.00)\n",
      "  degree         : 0.1900 (weight: 0.10)\n",
      "  finding        : 0.2000 (weight: 0.02)\n",
      "  location       : 0.6000 (weight: 0.20)\n",
      "  measurement    : 0.2800 (weight: 0.08)\n",
      "  presence       : 0.8000 (weight: 0.30)\n",
      "\n",
      "Entity Matching Summary:\n",
      "  Perfect matches: 2/5 (40.0%)\n",
      "  Partial matches: 3/5 (60.0%)\n",
      "  Missing entities: 0/5 (0.0%)\n",
      "  Extra entities: 0\n",
      "\n",
      "ðŸ’¾ Results saved: data/0/results/evaluation_results_final.json\n",
      "ðŸ’¾ Summary saved: data/0/results/summary_final.txt\n",
      "\n",
      "======================================================================\n",
      "BEST & WORST SAMPLES\n",
      "======================================================================\n",
      "\n",
      "Worst 2:\n",
      "  sample0.1.json: 0.300\n",
      "  sample0.2.json: 0.520\n",
      "\n",
      "Best 2:\n",
      "  sample0.3.json: 0.620\n",
      "  sample0.4.json: 0.931\n",
      "\n",
      "======================================================================\n",
      "FIELD-SPECIFIC ERRORS\n",
      "======================================================================\n",
      "\n",
      "ABNORMALITY:\n",
      "  sample0.0.json: 0.000\n",
      "  sample0.1.json: 0.000\n",
      "\n",
      "PRESENCE:\n",
      "  sample0.2.json: 0.000\n",
      "\n",
      "LOCATION:\n",
      "  sample0.1.json: 0.000\n",
      "  sample0.3.json: 0.000\n",
      "\n",
      "DEGREE:\n",
      "  sample0.0.json: 0.000\n",
      "  sample0.1.json: 0.000\n",
      "  sample0.2.json: 0.000\n",
      "  sample0.3.json: 0.200\n",
      "  sample0.4.json: 0.750\n",
      "\n",
      "MEASUREMENT:\n",
      "  sample0.0.json: 0.700\n",
      "  sample0.1.json: 0.000\n",
      "  sample0.2.json: 0.000\n",
      "  sample0.3.json: 0.000\n",
      "  sample0.4.json: 0.700\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize configuration\n",
    "config = EvaluationConfig()\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = ComprehensiveSchemaEvaluator(config)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_directory(\n",
    "    data_dir=config.DATA_DIR,\n",
    "    gt_filename=config.GT_FILE\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: CUSTOM ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Ã–rnek: En iyi ve en kÃ¶tÃ¼ Ã¶rnekleri bul\n",
    "struct_scores = [(r['sample_name'], r['structural']['overall_score']) for r in results]\n",
    "struct_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST & WORST SAMPLES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nWorst 2:\")\n",
    "for name, score in struct_scores[:2]:\n",
    "    print(f\"  {name}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nBest 2:\")\n",
    "for name, score in struct_scores[-2:]:\n",
    "    print(f\"  {name}: {score:.3f}\")\n",
    "\n",
    "# Ã–rnek: Field-specific error analysis\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FIELD-SPECIFIC ERRORS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for field_name in ['abnormality', 'presence', 'location', 'degree', 'measurement']:\n",
    "    print(f\"\\n{field_name.upper()}:\")\n",
    "    for r in results:\n",
    "        field_data = r['structural']['field_scores'].get(field_name, {})\n",
    "        mean_score = field_data.get('mean', 0)\n",
    "        if mean_score < 0.8:  # Show only problematic\n",
    "            print(f\"  {r['sample_name']}: {mean_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
