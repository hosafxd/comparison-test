comparison_test/
â”‚
â”œâ”€â”€ ğŸ“Š VERÄ°
â”‚   â”œâ”€â”€ data/0/
â”‚   â”‚   â”œâ”€â”€ gt0.json              â†’ Ground truth (doÄŸru cevap)
â”‚   â”‚   â””â”€â”€ sample0.X.json        â†’ Modelinizin tahminleri
â”‚   â”‚   â””â”€â”€ data/0_normalized/    â†’ Otomatik dÃ¼zeltilmiÅŸ versiyonlar
â”‚
â”œâ”€â”€ ğŸ§  CORE MODÃœLLERÄ° (Evaluation MekanizmalarÄ±)
â”‚   â”œâ”€â”€ medical_schema_evaluator.py
â”‚   â”‚   â””â”€â”€ Field-by-field karÅŸÄ±laÅŸtÄ±rma (structural)
â”‚   â”‚       â€¢ abnormality doÄŸru mu? âœ“/âœ—
â”‚   â”‚       â€¢ presence doÄŸru mu? âœ“/âœ—
â”‚   â”‚       â€¢ location eÅŸleÅŸiyor mu? âœ“/âœ—
â”‚   â”‚
â”‚   â”œâ”€â”€ multi_embedding_evaluator.py
â”‚   â”‚   â””â”€â”€ Semantic benzerlik (BioBERT, PubMedBERT...)
â”‚   â”‚       â€¢ "fracture" â‰ˆ "break" â†’ 0.95 similarity
â”‚   â”‚       â€¢ "fracture" â‰ˆ "effusion" â†’ 0.30 similarity
â”‚   â”‚
â”‚   â””â”€â”€ llm_evaluator.py
â”‚       â””â”€â”€ LLM ile klinik validasyon (Gemini, Gemma...)
â”‚           â€¢ "AynÄ± anlama mÄ± geliyor?" â†’ Yes/No
â”‚           â€¢ "Kritik hata var mÄ±?" â†’ List
â”‚
â”œâ”€â”€ ğŸ›ï¸ ORKESTRATÃ–RLERÄ° (Ana Kontrol)
â”‚   â”œâ”€â”€ comprehensive_evaluation.py
â”‚   â”‚   â””â”€â”€ TÃ¼m modelleri test et, sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
â”‚   â”‚
â”‚   â””â”€â”€ comprehensive_evaluation.ipynb
â”‚       â””â”€â”€ Notebook versiyonu (Ã§alÄ±ÅŸtÄ±rmak iÃ§in)
â”‚
â””â”€â”€ ğŸ“„ SONUÃ‡LAR
    â””â”€â”€ data/0_normalized/ulti_comp_results/
        â”œâ”€â”€ structural_only/          â†’ Sadece rule-based
        â”œâ”€â”€ embedding_biobert/        â†’ BioBERT semantic
        â”œâ”€â”€ llm_gemini_pro/          â†’ LLM validation
        â””â”€â”€ combined_X_Y/            â†’ Hepsi birlikte


        

INPUT: 
  GT: "fracture, present, distal radius, acute, 5mm"
  Prediction: "break, present, distal radius, sharp, about 5mm"

â†“

LEVEL 1: STRUCTURAL (medical_schema_evaluator.py)
  â”œâ”€ abnormality: "fracture" vs "break" â†’ FARKLI (0.0)
  â”œâ”€ presence: "present" vs "present" â†’ AYNI (1.0)
  â”œâ”€ location: "distal radius" vs "distal radius" â†’ AYNI (1.0)
  â”œâ”€ degree: "acute" vs "sharp" â†’ FARKLI (0.0)
  â””â”€ measurement: "5mm" vs "about 5mm" â†’ YAKIN (0.7)
  
  SKOR: 0.54 (weighted average)

â†“

LEVEL 2: SEMANTIC (multi_embedding_evaluator.py)
  Text1: "fracture present distal radius acute 5mm"
  Text2: "break present distal radius sharp about 5mm"
  
  BioBERT embedding â†’ Cosine similarity: 0.98
  (Ã‡Ã¼nkÃ¼ "fracture"â‰ˆ"break", "acute"â‰ˆ"sharp" synonym)

â†“

LEVEL 3: LLM (llm_evaluator.py)
  Gemini'ye sor: "Klinik olarak aynÄ± mÄ±?"
  
  Cevap: {
    "similarity": 0.7,
    "clinical_equivalence": "high",
    "critical_errors": ["degree mismatch"],
    "assessment": "Semantically equivalent, minor terminology difference"
  }

â†“

FINAL REPORT:
  Structural: 0.54 (katÄ± kurallar)
  Semantic: 0.98 (synonym yakalar)
  LLM: 0.70 (klinik bakÄ±ÅŸ)
  
  SONUÃ‡: Model iyi Ã§alÄ±ÅŸÄ±yor ama terminology standardize edilmeli
1. comprehensive_evaluation.ipynb aÃ§Ä±lÄ±r
   â†“
2. CELL 0: JSON normalization (otomatik dÃ¼zeltme)
   â†“
3. CELL 1: Evaluation baÅŸlat
   â†“
4. Loop: Her sample iÃ§in
   â”œâ”€ Structural eval
   â”œâ”€ Embedding eval (4 farklÄ± model)
   â””â”€ LLM eval (Gemini)
   â†“
5. SonuÃ§larÄ± kaydet:
   â”œâ”€ JSON (detaylÄ±)
   â”œâ”€ TXT (Ã¶zet)
   â””â”€ FINAL_REPORT.txt (karÅŸÄ±laÅŸtÄ±rma)



   
# Medical Schema Extraction - Evaluation Framework

A comprehensive, multi-level evaluation framework for medical information extraction systems, designed for academic research and publication.

## ğŸ¯ Overview

This framework evaluates medical schema extraction models using three complementary approaches:

1. **Structural Evaluation** - Rule-based field-by-field comparison
2. **Semantic Evaluation** - Embedding-based similarity using BioBERT
3. **Clinical Evaluation** - LLM-based assessment of clinical equivalence

## ğŸŒŸ Key Features

- âœ… **Duplicate Detection** - Prevents data leakage in train/test splits
- âœ… **Stratified Splitting** - Groups by input to avoid information leakage
- âœ… **Multi-Level Scoring** - Weighted field importance for clinical relevance
- âœ… **Statistical Validation** - Bootstrap confidence intervals & significance tests
- âœ… **LLM Integration** - Uses free APIs (Gemini, HuggingFace, Together AI)
- âœ… **Comprehensive Reporting** - JSON + human-readable summaries

## ğŸ“‹ Schema Format

Input schema format (same as schema_train.json):

```json
{
  "instruction": "Extract medical entities...",
  "input": "No fracture observed in the skull.",
  "output": [
    {
      "abnormality": "fracture",
      "finding": "None",
      "presence": "absent",
      "location": ["skull"],
      "degree": "None",
      "measurement": "None",
      "comparison": "None"
    }
  ]
}
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone or download the framework
cd medical-schema-evaluation

# Install dependencies
pip install -r requirements.txt

# Install sentence-transformers for semantic evaluation
pip install sentence-transformers
```

### 2. Setup API Keys (Optional but Recommended)

For LLM-based evaluation, get a free API key:

**Option A: Google Gemini (Recommended - 2M tokens/day FREE)**
```bash
# Get key from: https://aistudio.google.com/app/apikey
export GEMINI_API_KEY="your-api-key-here"
```

**Option B: HuggingFace**
```bash
# Get key from: https://huggingface.co/settings/tokens
export HUGGINGFACE_API_KEY="your-api-key-here"
```

**Option C: Together AI**
```bash
# Get key from: https://api.together.xyz/
export TOGETHER_API_KEY="your-api-key-here"
```

### 3. Prepare Your Data

You need two files:

1. **Ground Truth**: `schema_train.json` (your uploaded file)
2. **Predictions**: Your model's output in the same format

Example prediction file (`model_predictions.json`):
```json
[
  {
    "instruction": "Extract medical entities...",
    "input": "No fracture observed in the skull.",
    "output": [
      {
        "abnormality": "fracture",
        "finding": "None",
        "presence": "absent",
        "location": ["cranial bones"],
        "degree": "None",
        "measurement": "None",
        "comparison": "None"
      }
    ]
  }
]
```

### 4. Run Evaluation

```python
from evaluation_pipeline import ComprehensiveEvaluationPipeline

# Initialize pipeline
pipeline = ComprehensiveEvaluationPipeline(
    use_llm=True,           # Enable LLM evaluation
    use_embeddings=True,    # Enable semantic similarity
    llm_provider='gemini'   # or 'huggingface' or 'together'
)

# Run complete evaluation
results = pipeline.run_complete_evaluation(
    ground_truth_path='schema_train.json',
    predictions_path='model_predictions.json',
    output_dir='./evaluation_results',
    test_size=0.2,          # 20% for testing
    random_seed=42          # For reproducibility
)

# Results are automatically saved to output_dir
```

Or run from command line:
```bash
python evaluation_pipeline.py
```

## ğŸ“Š Evaluation Metrics

### Structural Metrics
- **F1 Score**: Harmonic mean of precision and recall
- **Field-wise Accuracy**: Individual accuracy for each field
  - `abnormality` (weight: 0.25)
  - `presence` (weight: 0.25) - Critical for clinical decisions
  - `location` (weight: 0.20)
  - `degree` (weight: 0.15)
  - `measurement` (weight: 0.10)
- **Exact Match Rate**: Percentage of perfect predictions

### Semantic Metrics
- **Cosine Similarity**: Using BioBERT embeddings
- Captures synonyms and paraphrasing

### Clinical Metrics (LLM-based)
- **Clinical Equivalence**: High/Medium/Low/None
- **Clinical Impact**: Critical/Moderate/Minor/None
- **Detailed Explanations**: Per-entity analysis

### Statistical Metrics
- **Bootstrap Confidence Intervals** (95%)
- **Paired t-tests** for model comparison
- **Cohen's d** effect size

## ğŸ“ Output Files

After running evaluation, you'll get:

```
evaluation_results/
â”œâ”€â”€ evaluation_report_TIMESTAMP.json    # Main results
â”œâ”€â”€ structural_results_TIMESTAMP.json  # Detailed scores
â”œâ”€â”€ llm_results_TIMESTAMP.json         # LLM evaluations
â”œâ”€â”€ summary_TIMESTAMP.txt              # Human-readable
â””â”€â”€ duplicates_TIMESTAMP.json          # Duplicate analysis
```

## ğŸ”¬ Academic Usage

### For Your Paper

This framework provides:

1. **Rigorous Methodology**
   - Proper train/test splitting with no leakage
   - Statistical significance testing
   - Multiple evaluation perspectives

2. **Reproducibility**
   - Fixed random seeds
   - Documented configurations
   - Complete error analysis

3. **Comprehensive Metrics**
   - Rule-based (objective)
   - Embedding-based (semantic)
   - LLM-based (clinical validity)

### Citation-Worthy Features

- **Multi-level Evaluation**: Novel combination of rule-based, semantic, and LLM approaches
- **Clinical Weighting**: Field importance based on clinical relevance
- **Duplicate-Aware Splitting**: Prevents overoptimistic performance estimates
- **Free & Reproducible**: All tools are free and open-source

### Suggested Methodology Section

```
We evaluated our model using a comprehensive three-level framework:

1. Structural Evaluation: Rule-based field-by-field comparison with 
   clinically-weighted importance scores.

2. Semantic Evaluation: BioBERT embeddings (dmis-lab/biobert-base-cased-v1.2) 
   to capture semantic similarity and handle paraphrasing.

3. Clinical Validation: LLM-based assessment using Gemini 1.5 Flash to 
   determine clinical equivalence and identify critical vs. minor errors.

Data splitting employed stratification by input text to prevent information 
leakage from duplicate samples. Statistical significance was established using 
bootstrap confidence intervals (10,000 iterations) and paired t-tests.
```

## ğŸ§ª Advanced Usage

### Custom Field Weights

```python
from medical_schema_evaluator import MedicalSchemaEvaluator

evaluator = MedicalSchemaEvaluator()

# Customize weights based on your clinical focus
evaluator.FIELD_WEIGHTS = {
    'abnormality': 0.30,  # Increase if abnormality detection is critical
    'presence': 0.30,
    'location': 0.20,
    'degree': 0.10,
    'measurement': 0.10,
}
```

### Statistical Comparison of Models

```python
from medical_schema_evaluator import StatisticalAnalyzer

# Compare two models
baseline_scores = [0.85, 0.82, 0.88, ...]  # Model A
new_model_scores = [0.89, 0.87, 0.91, ...]  # Model B

comparison = StatisticalAnalyzer.compare_models(
    baseline_scores,
    new_model_scores
)

print(f"Improvement: {comparison['improvement']:.4f}")
print(f"P-value: {comparison['p_value']:.4f}")
print(f"Significant: {comparison['significant']}")
```

### Custom LLM Evaluation Prompt

```python
from llm_evaluator import LLMEvaluator

evaluator = LLMEvaluator(provider='gemini')

# Modify the prompt in llm_evaluator.py:_build_evaluation_prompt()
# to focus on specific aspects relevant to your research
```

## ğŸ› Troubleshooting

### "No module named 'sentence_transformers'"
```bash
pip install sentence-transformers
```

### "API key not found"
```bash
# Set environment variable
export GEMINI_API_KEY="your-key"

# Or set in Python
import os
os.environ['GEMINI_API_KEY'] = 'your-key'
```

### "Rate limit exceeded"
- Use `time.sleep()` between API calls
- Reduce batch size in `batch_evaluate()`
- Switch to a different provider

### "Model download failed"
```python
# Download models manually
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
```

## ğŸ“ˆ Performance Optimization

### For Large Datasets

```python
# 1. Sample for LLM evaluation (most expensive)
sample_size = 50  # Instead of evaluating all

# 2. Disable LLM for initial testing
pipeline = ComprehensiveEvaluationPipeline(use_llm=False)

# 3. Cache embeddings
# Embeddings are computed once and can be reused

# 4. Parallel processing (future enhancement)
# Can parallelize structural evaluation across samples
```

## ğŸ¤ Contributing

This is a research framework. Suggestions welcome:

- Additional evaluation metrics
- Support for more LLM providers
- Visualization tools
- Cross-validation strategies

## ğŸ“š References

### Models & APIs Used

- **BioBERT**: Lee et al. (2020), "BioBERT: a pre-trained biomedical language representation model"
- **Sentence-BERT**: Reimers & Gurevych (2019), "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
- **Gemini 1.5**: Google DeepMind, "Gemini: A Family of Highly Capable Multimodal Models"

### Evaluation Methodology

- Bootstrap confidence intervals: Efron & Tibshirani (1993)
- Cohen's d effect size: Cohen (1988)
- Inter-annotator agreement: Krippendorff's alpha

## ğŸ“ License

This framework is provided for research and academic use.

## ğŸ™‹ Support

For questions or issues:
1. Check the troubleshooting section
2. Review the example code
3. Consult the inline documentation in source files

---

**Good luck with your research! ğŸš€**
